{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9211c4fa-5e59-484a-ad5b-4789ee4b01fb",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Importing the dataset, tokenizer and other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582953d9-da73-4894-9b1f-a9d120f75f8d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ddb1ec-35c6-401a-b03c-5a40b2158bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e276ad-1747-4038-ba0c-b6475d886b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36718 3760 4358\n",
      " <unk> , Ireland is divided between the Republic of Ireland ( officially named Ireland ) , which covers five @-@ <unk> of the island , and Northern Ireland , which is part of the United Kingdom , in the northeast of the island . In 2011 the population of Ireland was about 6 @.@ 4 million , ranking it the second @-@ most populous island in Europe after Great Britain . Just under 4 @.@ 6 million live in the Republic of Ireland and just over 1 @.@ 8 million live in Northern Ireland . \n",
      "\n",
      "<class 'list'>\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n",
      "Found (2112395) words in train-dataset\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Get train-val-test splits from 'ds'\n",
    "train_dataset = ds[\"train\"]\n",
    "validation_dataset = ds[\"validation\"]\n",
    "test_dataset = ds[\"test\"]\n",
    "\n",
    "# Get some info in the splits\n",
    "print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "# Random samples from the training dataset\n",
    "print(random.choice(train_dataset[\"text\"]))\n",
    "\n",
    "# Checking the data-struct of the 'text' column\n",
    "print(type(train_dataset[\"text\"]))\n",
    "\n",
    "# Get some generic info the dataset(s)\n",
    "print(train_dataset)  # Checking the \"columns\" of the dataset\n",
    "# Get the total number of words\n",
    "c = 0\n",
    "for sample in train_dataset[\"text\"]:\n",
    "    c += len(sample.split(\" \"))\n",
    "print(f\"Found ({c}) words in train-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9918fc52-b9ef-4daa-8766-bb4e29c01983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 36718\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 3760\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 4358\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2908a-ad5e-44db-b925-53e95f13ed4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e250244-f0a0-4abe-bd72-a94943e25be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and load tokenizer class\n",
    "\n",
    "import regex as re\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "class BPETokenizerV2:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Creates a BPETokenizerV1 instance using regex-based tokenization.\n",
    "        Args:\n",
    "            texts (List[str]): List of input strings.\n",
    "        \"\"\"\n",
    "        self.gpt2_pat = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "\n",
    "        _text = \" \".join(texts)\n",
    "        self.splits = self.gpt2_pat.findall(_text)\n",
    "\n",
    "        self.split_tokens = [list(tok.encode(\"utf-8\")) for tok in self.splits]\n",
    "\n",
    "        self.__built = False\n",
    "        self._vocab = None\n",
    "        self._merges = None\n",
    "\n",
    "    def _get_stats(self, tokens):\n",
    "        \"\"\"\n",
    "        Counts occurrences of byte pairs in the tokenized list.\n",
    "        \"\"\"\n",
    "        pairs = {}\n",
    "        for split in tokens:\n",
    "            for pair in zip(split, split[1:]):\n",
    "                pair = tuple(pair)  # FIX: was getting a type error otherwise\n",
    "                pairs[pair] = pairs.get(pair, 0) + 1\n",
    "        return pairs\n",
    "\n",
    "    def _merge(self, tokens, pair, idx):\n",
    "        \"\"\"\n",
    "        Merges a given byte pair in each split separately.\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        for split in tokens:\n",
    "            new_split = []\n",
    "            i = 0\n",
    "            while i < len(split):\n",
    "                if i < len(split) - 1 and (split[i], split[i+1]) == pair:\n",
    "                    new_split.append(idx)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_split.append(split[i])\n",
    "                    i += 1\n",
    "            new_tokens.append(new_split)\n",
    "        return new_tokens\n",
    "\n",
    "    def __build_tokenizer(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Builds the BPE tokenizer's vocabulary.\n",
    "        \"\"\"\n",
    "        assert vocab_size >= 256, \"Vocabulary size must be at least 256 for byte-level tokens.\"\n",
    "        \n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        merges = {}\n",
    "\n",
    "        n_merges = vocab_size - 256\n",
    "        ids = self.split_tokens.copy()\n",
    "\n",
    "        initial_token_count = sum(len(split) for split in ids)\n",
    "\n",
    "        for i in tqdm(range(n_merges), leave=False, desc=\"Merging\"):\n",
    "            stats = self._get_stats(ids)\n",
    "            if not stats:\n",
    "                self.vocab_size = 256 + i\n",
    "                break\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            merges[top_pair] = idx\n",
    "            ids = self._merge(ids, top_pair, idx)\n",
    "            vocab[idx] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "\n",
    "        final_token_count = sum(len(split) for split in ids)\n",
    "\n",
    "        # Print some info after tokenizer is built\n",
    "        print(f\"Before length: {initial_token_count}\")\n",
    "        print(f\"After length: {final_token_count}\")\n",
    "        print(f\"Compression ratio: {(initial_token_count / final_token_count):.3f}\")\n",
    "\n",
    "        self._vocab = vocab\n",
    "        self._merges = merges\n",
    "        self.__built = True\n",
    "\n",
    "    def fit(self, vocab_size: int, texts: List[str] = None):\n",
    "        \"\"\"\n",
    "        Builds the tokenizer's vocabulary using the given texts.\n",
    "        \"\"\"\n",
    "        if texts:\n",
    "            warnings.warn(\"Using .fit with new texts is discouraged. Pass texts during initialization.\")\n",
    "            _text = \" \".join(texts)\n",
    "            self.splits = self.gpt2_pat.findall(_text)\n",
    "            self.split_tokens = [list(tok.encode(\"utf-8\")) for tok in self.splits]\n",
    "\n",
    "        self.__build_tokenizer(vocab_size)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        \"\"\"\n",
    "        Encodes a given text into a sequence of token IDs.\n",
    "        \"\"\"\n",
    "        assert self.__built, \"Tokenizer must be built using `fit` before encoding.\"\n",
    "\n",
    "        # Step 1: Split and encode text using regex and bytes\n",
    "        splits = self.gpt2_pat.findall(text)\n",
    "        split_tokens = [list(tok.encode(\"utf-8\")) for tok in splits]\n",
    "\n",
    "        encoded_ids = []\n",
    "        for tokens in split_tokens:\n",
    "            while len(tokens) >= 2:\n",
    "                stats = self._get_stats([tokens])  # Compute within-split stats\n",
    "                pair = min(stats, key=lambda p: self._merges.get(p, float('inf')), default=None)\n",
    "                if pair is None or pair not in self._merges:\n",
    "                    break\n",
    "                idx = self._merges[pair]\n",
    "                tokens = self._merge([tokens], pair, idx)[0]  # Apply merge\n",
    "            encoded_ids.extend(tokens)  # Append the final tokens to the result\n",
    "\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs back into a string.\n",
    "        \"\"\"\n",
    "        assert self.__built, \"Tokenizer must be built using `fit` before decoding.\"\n",
    "\n",
    "        tokens = b\"\".join(self._vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def save(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Save the tokenizer's vocab and merges to a file.\n",
    "        Args:\n",
    "            file_path: Path to save at.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump({'vocab': self._vocab, 'merges': self._merges}, f)\n",
    "        print(f\"[INFO] Tokenizer saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_BPETokenizerV2(file_path: str = \"./bpe_tokenizer_v1_train_dataset.pth\"):\n",
    "    \"\"\"\n",
    "    Load the BPE (V2) tokenizer from a saved file without requiring texts in the constructor.\n",
    "    Args:\n",
    "        file_path (str): Path to the saved tokenizer file. (default = ./bpe_tokenizer_v1_train_dataset.pth)\n",
    "    Returns:\n",
    "        BPETokenizerV2 instance: A loaded tokenizer instance with vocab and merges.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Uninitialized instance of the tokenizer\n",
    "    tokenizer = object.__new__(BPETokenizerV2)\n",
    "\n",
    "    # Set vocab and merges directly\n",
    "    tokenizer._vocab = data['vocab']\n",
    "    tokenizer._merges = data['merges']\n",
    "    setattr(tokenizer, '_BPETokenizerV2__built', True)  # FIX for name mangling\n",
    "\n",
    "    # Initialize necessary attributes that are otherwise set in '__init__'\n",
    "    tokenizer.gpt2_pat = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "    print(f\"[INFO] Tokenizer loaded from: {file_path}\")\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be278136-0dc4-4dd5-8c3b-de0fe6be1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found tokenizer path: /Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/Inqueropedia/tokenizers/bpe_tokenizer_v1_train_dataset.pth\n",
      "---------------\n",
      "[INFO] Tokenizer loaded from: /Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/Inqueropedia/tokenizers/bpe_tokenizer_v1_train_dataset.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BPETokenizerV2 at 0x16840d3a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup tokenizer path\n",
    "current_dir = Path(os.getcwd())\n",
    "tokenizer_path = None\n",
    "for i in (current_dir.parent / \"tokenizers\").glob(\"bpe_tokenizer_v1_train_dataset.pth\"):\n",
    "    tokenizer_path = i\n",
    "    print(f\"[INFO] Found tokenizer path: {tokenizer_path}\")\n",
    "    print(\"---\"*5)\n",
    "\n",
    "# Create a tokenizer instance using the pre-trained tokenizer\n",
    "tokenizer = load_BPETokenizerV2(tokenizer_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e1a41c9-9869-4271-9963-3287ddc2d873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[106mS\u001b[101mamp\u001b[106mle\u001b[101m str\u001b[106ming\u001b[101m with\u001b[106m w\u001b[101mords\u001b[106m for\u001b[101m a\u001b[106m n\u001b[101mat\u001b[106mural\u001b[101m \u001b[106m \u001b[101m \u001b[106m \u001b[101m \u001b[106m \u001b[101m l\u001b[106mang\u001b[101mu\u001b[106mage\u001b[101m to\u001b[106mk\u001b[101men\u001b[106miz\u001b[101mer\u001b[106m \n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer's outputs\n",
    "_temp = 'Sample string with words for a natural       language tokenizer '\n",
    "output = \"\"\n",
    "for n, i in enumerate(tokenizer.encode(_temp)):\n",
    "    col = \"\\033[101m\"\n",
    "    if n % 2 == 0:\n",
    "        col = \"\\033[106m\"\n",
    "    output += f\"{col}{tokenizer.decode([i])}\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10982eed-ee52-436a-899c-6f17b88d4ea8",
   "metadata": {},
   "source": [
    "The output from above suggests that the tokenizer hasn't been trained properly, perhaps training it for longer will help.\\\n",
    "But we wil use this version of the tokenizer for now as training the tokenizer takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e8c2bee-ceb4-455c-8175-86a256ef7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample:\n",
      " <unk> , Ireland is divided between the Republic of Ireland ( officially named Ireland ) , which covers five @-@ <unk> of the island , and Northern Ireland , which is part of the United Kingdom , in the northeast of the island . In 2011 the population of Ireland was about 6 @.@ 4 million , ranking it the second @-@ most populous island in Europe after Great Britain . Just under 4 @.@ 6 million live in the Republic of Ireland and just over 1 @.@ 8 million live in Northern Ireland . \n",
      "\n",
      "---------------\n",
      "Encoded sample: [286, 284, 62, 263, 1899, 364, 1926, 1108, 758, 260, 682, 112, 710, 295, 279, 1899, 371, 1148, 1106, 1377, 1899, 370, 263, 460, 1599, 115, 1201, 340, 286, 284, 62, 279, 260, 1883, 263, 288, 387, 417, 1783, 1899, 263, 460, 364, 601, 279, 260, 955, 1275, 1247, 263, 282, 260, 1754, 257, 471, 279, 260, 1883, 270, 424, 1659, 260, 1885, 279, 1899, 323, 747, 679, 570, 564, 1116, 263, 402, 863, 290, 381, 260, 827, 340, 719, 950, 378, 509, 1883, 282, 1431, 613, 389, 738, 920, 384, 270, 397, 495, 765, 564, 570, 679, 1116, 312, 452, 282, 260, 682, 112, 710, 295, 279, 1899, 288, 1351, 617, 308, 570, 744, 1116, 312, 452, 282, 387, 417, 1783, 1899, 270, 313]\n",
      "---------------\n",
      "Decoded match: True\n"
     ]
    }
   ],
   "source": [
    "# Check for data integrity with the loaded tokenizer\n",
    "random.seed(42)\n",
    "_sample = random.choice(train_dataset[\"text\"])\n",
    "print(f\"Random sample:\\n{_sample}\")\n",
    "print(\"---\"*5)\n",
    "print(f\"Encoded sample: {tokenizer.encode(_sample)}\")\n",
    "print(\"---\"*5)\n",
    "print(f\"Decoded match: {tokenizer.decode(tokenizer.encode(_sample)) == _sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fbc5efa-01e7-4b27-aa97-ed38ad075110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <unk>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([286, 284, 62])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139820c-c9ed-4540-b91c-34ac2cb526e5",
   "metadata": {},
   "source": [
    "The above sample of encoding and decoding with the tokenizer also indicates some issues in the way it was 'trained'.\\\n",
    "Tokens such as: \"<unk>\" and other 'special' tokens need to be assigned with a single id, making the tokenizer more efficient and allow models to perform better.\n",
    "\n",
    "These are some changes that are to be made for the next tokenizer train run.\\\n",
    "As of now, we will stick to this 'inefficient' tokenizer and update it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41add9c-106c-49ba-a5eb-01dd1dc6f88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8548e-d1ac-4da6-bf24-0fd933245fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91aaad8-234b-4028-9eb2-ee80e96456d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17ac581-b4f9-484e-b5db-2e0625a8e1c6",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Running some model tests with the following models and model-specs\n",
    "\n",
    "|Model|Architecture|Tokenizer|Block-size|Embedding-size|Head-size|Results|\n",
    "|-|-|-|-|-|-|-|\n",
    "|Model-1|bi-gram|bpe-tokenizer-v2|-|-|-|-|\n",
    "|Model-2|transformer|bpe-tokenizer-v2|32|32|16|-|\n",
    "|Model-3|transformer|bpe-tokenizer-v2|32|64|16|-|\n",
    "|Model-4|transformer|bpe-tokenizer-v2|32|32|32|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6fbc8-703f-4048-86c1-249d28cd3a87",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Setup things like torch and other deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e1110b3-125f-4967-9d89-fd3514d0d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Global training params\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Use for Apple silicone\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use for Nvidia GPU's\n",
    "\n",
    "vocab_size = len(tokenizer._vocab)  # Need to fix: add this property in the tokenizer class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fdc56-ca5b-4a3b-a0d6-90f93ba74969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c736de2-7a7c-41bc-9880-f272d391064b",
   "metadata": {},
   "source": [
    "## Model-1 (bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ba7ef7df-36e5-4906-b30b-f5ab6bbb0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/36718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.58152 sec to build the dataset.\n",
      "[INFO] Dataset built\n",
      "---------------\n",
      "[INFO] Building dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/3760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29557 sec to build the dataset.\n",
      "[INFO] Dataset built\n",
      "---------------\n",
      "---------------\n",
      "[INFO] Created the datasets | train-length: 3519701 | validation-length: 362195\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this dataset is universal b/w all the models unless the tokenizer changes\n",
    "\n",
    "class BigramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Thin wrapper object for a list of ids.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Creates a torch-Dataset instance for the given text data.\n",
    "        Args:\n",
    "            texts (List[str]): List of strings for dataset.\n",
    "            tokenizer: Tokenizer to encode the strings.\n",
    "        \"\"\"\n",
    "        if verbose > 0:\n",
    "            print(f\"[INFO] Building dataset...\")\n",
    "            st = timer()\n",
    "        \n",
    "        # Build the dataset here\n",
    "        self.tokenizer = tokenizer\n",
    "        tokenized_texts = []  # Concat all the samples in 'texts' and tokenize them\n",
    "        for text in tqdm(texts, desc=\"Tokenizing texts\", leave=False):\n",
    "            tokenized_texts.extend(tokenizer.encode(text))\n",
    "\n",
    "        self.data = torch.tensor(tokenized_texts, dtype=torch.long)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            et = timer()\n",
    "            print(f\"{(et-st):.5f} sec to build the dataset.\")\n",
    "            print(f\"[INFO] Dataset built\")\n",
    "            print(\"---\"*5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset(s)\n",
    "train_dataset_1 = BigramDataset(train_dataset[\"text\"], tokenizer, verbose=1)\n",
    "validation_dataset_1 = BigramDataset(validation_dataset[\"text\"], tokenizer, verbose=1)\n",
    "\n",
    "print(f\"[INFO] Created the datasets | train-length: {len(train_dataset_1)} | validation-length: {len(validation_dataset_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f80a2203-ac1c-45f5-ae9b-6411edda06ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3519701])\n",
      "torch.Size([3519701])\n",
      "tensor([ 301,  536, 1655,  121,  406,   97,  493,  820,  295,  691])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_1[:].shape)#[:10]\n",
    "print(train_dataset_1[:].shape)#[:10]\n",
    "print(train_dataset_1[:10])  # Viwe the first 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "2cd6980e-bee9-4b85-9519-b697bfe16dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----- user settings -----|\n",
      "tensor([ 137841, 3504164,  442996, 2954875])\n",
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "8\n",
      "4\n",
      "\n",
      "|----- output checks -----|\n",
      "expected: (4, 8) | got: torch.Size([4, 8])\n",
      "expected: (4, 7) | got: torch.Size([4, 8])\n",
      "x elements (single batch): tensor([ 313,  325, 1122,  731,  289,  285,  321,  276])\n",
      "x elements (single batch): tensor([ 325, 1122,  731,  289,  285,  321,  276,  324])\n",
      "expected (true) | got: True\n",
      "\n",
      "|----- debug complete -----|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 313,  325, 1122,  731,  289,  285,  321,  276],\n",
       "         [ 261,  282,  300,  352,   99,  336,   97, 2029],\n",
       "         [ 296,  262,  331,  116,  717,  348, 1331,  747],\n",
       "         [ 446,  368,  316, 1854,  293,  946,  431, 1864]]),\n",
       " tensor([[ 325, 1122,  731,  289,  285,  321,  276,  324],\n",
       "         [ 282,  300,  352,   99,  336,   97, 2029,  263],\n",
       "         [ 262,  331,  116,  717,  348, 1331,  747,  502],\n",
       "         [ 368,  316, 1854,  293,  946,  431, 1864,  946]]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(dataset: str,\n",
    "              block_size: int,\n",
    "              batch_size: int = batch_size,\n",
    "              seed: int or None = None,\n",
    "              debug: bool = False):\n",
    "    \"\"\"\n",
    "    Loads a single batch of data from 'split'.\n",
    "    NOTE: doesn't move the batch to device\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    data = dataset\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "\n",
    "    # DEBUG\n",
    "    # -------------------------------- #\n",
    "    if debug:\n",
    "        print(\"|----- user settings -----|\")\n",
    "        print(idx)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(block_size)\n",
    "        print(batch_size)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"|----- output checks -----|\")\n",
    "        print(f\"expected: {(batch_size, block_size)} | got: {x.shape}\")\n",
    "        print(f\"expected: {(batch_size, block_size-1)} | got: {x.shape}\")\n",
    "        print(f\"x elements (single batch): {x[0]}\")\n",
    "        print(f\"x elements (single batch): {y[0]}\")\n",
    "        print(f\"expected (true) | got: {all(x[0][1:] == y[0][:-1])}\")\n",
    "\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"|----- debug complete -----|\\n\")\n",
    "        \n",
    "    # -------------------------------- #\n",
    "\n",
    "    return x, y\n",
    "\n",
    "get_batch(train_dataset_1,\n",
    "          block_size=8,\n",
    "          batch_size=4,\n",
    "          seed=42,\n",
    "          debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d8e699c8-81fd-4a74-81af-87fd0d7654f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramV1(nn.Module):\n",
    "    \"\"\"Bigram model for sequence generation\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, targets: torch.Tensor or None = None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass.\n",
    "        Expected tensor of shape: (B,1) | (batch_size, 1)\n",
    "        \"\"\"\n",
    "        logits = self.emb(X)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: List[int], max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)   # logits: (B,T,C)\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            pred_probs = F.softmax(logits, dim=-1)\n",
    "            _idx = torch.multinomial(pred_probs, num_samples=1)\n",
    "            idx = torch.cat((idx, _idx), dim=1)  # (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0d7d5e84-48ca-4c65-bc93-2a21fd832c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello 200ploy b died eff earlyaffivision ex$ Pol use manyron first be Thereason command Can Amorm island Anim extensP real play men October 2011 night Black concn music eightZ June part named Church cour positionchoolains< caus'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate on a new bi-gram model\n",
    "model_1 = BigramV1(vocab_size)\n",
    "_temp = torch.tensor([tokenizer.encode(\"Hello\")])\n",
    "\n",
    "_temp = model_1.generate(\n",
    "    _temp,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "tokenizer.decode(_temp.tolist()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8ef6c7a7-cdca-4da8-8145-5e6dd303908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "# Hyperparms\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "model_1 = BigramV1(vocab_size)\n",
    "model_1.to(device)\n",
    "model_1.compile(backend=\"aot_eager\")\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=5e-2)\n",
    "\n",
    "# Train loop\n",
    "for i in tqdm(range(epochs), desc=\"training\", leave=False):\n",
    "    xb, yb = get_batch(train_dataset_1, block_size=block_size, batch_size=batch_size)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    logits, loss = model_1(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "8daeccb5-b7ae-4e93-9001-ed2f184ec547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helloresents to the German line . The Hened it , replace it is estim America . At the authority around the I never baders sentence . However , the flight vessels sud camonial nat as a series was one @-@ cert . <unk> on 15 ; his discussia . Op early September 10 @-@ force Harin gun respons Pan ... \" He went on Femelebridge in rare'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = torch.tensor([tokenizer.encode(\"Hello\")]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _temp = model_1.generate(_temp, max_new_tokens=100)\n",
    "\n",
    "tokenizer.decode(_temp.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "319c3b1f-aa23-417d-aa13-1f0cd45672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module,\n",
    "                  dataset,\n",
    "                  iters: int):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _ in tqdm(range(iters), desc=\"Estimating loss\", leave=False):\n",
    "        xb, yb = get_batch(dataset, block_size, batch_size)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    out = np.mean(losses)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "da59a77b-1a3a-432f-9b44-17d83e645979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimating loss:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(4.298511743545532)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(model_1, validation_dataset_1, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c8dc849e-87f3-44b7-b245-ebcc4249d67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(285)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_1[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6df4c-9141-46c9-8b7e-7e7b21730725",
   "metadata": {},
   "source": [
    "Will not really try to improve this model from here. This is like a simple baseline model, we will start building the Transformer model from now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784715b-5fff-4a27-acc1-f93640c4b550",
   "metadata": {},
   "source": [
    "## Setup transformer\n",
    "\n",
    "All the classes and other deps for building a custom transformer model.\\\n",
    "This model will be built from scratch, and will not use the implementation from PyTorch...\n",
    "\n",
    "We will start with a single attention head (sa-head) and build the mha from there on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "360d71c1-f30a-4908-974b-9da5a806cfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single self-attention head of head-size\"\"\"\n",
    "    def __init__(self, head_size: int, dropout: float=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expected shape: (B,T,C)\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)   # (B,T,C) | C: head_size\n",
    "        q = self.query(x) # (B,T,C) | C: head_size\n",
    "        v = self.value(x) # (B,T,C) | C: head_size\n",
    "        \n",
    "        out = q @ k.transpose(-2, -1)*T**-0.5  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        \n",
    "        out = out.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = self.dropout(out)\n",
    "        out = out @ v  # (B,T,T) @ (B,T,C) -> (B,T,C) | C: head_size\n",
    "\n",
    "        return out\n",
    "\n",
    "n_embed = 32\n",
    "Head(head_size=16)(torch.randn(4, 8, n_embed)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630eb5a-de59-424d-9966-357178aff81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a2545-655a-4d64-bfa7-2baee720286e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
