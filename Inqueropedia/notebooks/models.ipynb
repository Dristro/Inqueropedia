{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9211c4fa-5e59-484a-ad5b-4789ee4b01fb",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Importing the dataset, tokenizer and other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582953d9-da73-4894-9b1f-a9d120f75f8d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ddb1ec-35c6-401a-b03c-5a40b2158bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e276ad-1747-4038-ba0c-b6475d886b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36718 3760 4358\n",
      " <unk> , Ireland is divided between the Republic of Ireland ( officially named Ireland ) , which covers five @-@ <unk> of the island , and Northern Ireland , which is part of the United Kingdom , in the northeast of the island . In 2011 the population of Ireland was about 6 @.@ 4 million , ranking it the second @-@ most populous island in Europe after Great Britain . Just under 4 @.@ 6 million live in the Republic of Ireland and just over 1 @.@ 8 million live in Northern Ireland . \n",
      "\n",
      "<class 'list'>\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n",
      "Found (2112395) words in train-dataset\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Get train-val-test splits from 'ds'\n",
    "train_dataset = ds[\"train\"]\n",
    "validation_dataset = ds[\"validation\"]\n",
    "test_dataset = ds[\"test\"]\n",
    "\n",
    "# Get some info in the splits\n",
    "print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "# Random samples from the training dataset\n",
    "print(random.choice(train_dataset[\"text\"]))\n",
    "\n",
    "# Checking the data-struct of the 'text' column\n",
    "print(type(train_dataset[\"text\"]))\n",
    "\n",
    "# Get some generic info the dataset(s)\n",
    "print(train_dataset)  # Checking the \"columns\" of the dataset\n",
    "# Get the total number of words\n",
    "c = 0\n",
    "for sample in train_dataset[\"text\"]:\n",
    "    c += len(sample.split(\" \"))\n",
    "print(f\"Found ({c}) words in train-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9918fc52-b9ef-4daa-8766-bb4e29c01983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 36718\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 3760\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 4358\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2908a-ad5e-44db-b925-53e95f13ed4a",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e250244-f0a0-4abe-bd72-a94943e25be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and load tokenizer class\n",
    "\n",
    "import regex as re\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "class BPETokenizerV2:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Creates a BPETokenizerV1 instance using regex-based tokenization.\n",
    "        Args:\n",
    "            texts (List[str]): List of input strings.\n",
    "        \"\"\"\n",
    "        self.gpt2_pat = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "\n",
    "        _text = \" \".join(texts)\n",
    "        self.splits = self.gpt2_pat.findall(_text)\n",
    "\n",
    "        self.split_tokens = [list(tok.encode(\"utf-8\")) for tok in self.splits]\n",
    "\n",
    "        self.__built = False\n",
    "        self._vocab = None\n",
    "        self._merges = None\n",
    "\n",
    "    def _get_stats(self, tokens):\n",
    "        \"\"\"\n",
    "        Counts occurrences of byte pairs in the tokenized list.\n",
    "        \"\"\"\n",
    "        pairs = {}\n",
    "        for split in tokens:\n",
    "            for pair in zip(split, split[1:]):\n",
    "                pair = tuple(pair)  # FIX: was getting a type error otherwise\n",
    "                pairs[pair] = pairs.get(pair, 0) + 1\n",
    "        return pairs\n",
    "\n",
    "    def _merge(self, tokens, pair, idx):\n",
    "        \"\"\"\n",
    "        Merges a given byte pair in each split separately.\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        for split in tokens:\n",
    "            new_split = []\n",
    "            i = 0\n",
    "            while i < len(split):\n",
    "                if i < len(split) - 1 and (split[i], split[i+1]) == pair:\n",
    "                    new_split.append(idx)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_split.append(split[i])\n",
    "                    i += 1\n",
    "            new_tokens.append(new_split)\n",
    "        return new_tokens\n",
    "\n",
    "    def __build_tokenizer(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Builds the BPE tokenizer's vocabulary.\n",
    "        \"\"\"\n",
    "        assert vocab_size >= 256, \"Vocabulary size must be at least 256 for byte-level tokens.\"\n",
    "        \n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        merges = {}\n",
    "\n",
    "        n_merges = vocab_size - 256\n",
    "        ids = self.split_tokens.copy()\n",
    "\n",
    "        initial_token_count = sum(len(split) for split in ids)\n",
    "\n",
    "        for i in tqdm(range(n_merges), leave=False, desc=\"Merging\"):\n",
    "            stats = self._get_stats(ids)\n",
    "            if not stats:\n",
    "                self.vocab_size = 256 + i\n",
    "                break\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            merges[top_pair] = idx\n",
    "            ids = self._merge(ids, top_pair, idx)\n",
    "            vocab[idx] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "\n",
    "        final_token_count = sum(len(split) for split in ids)\n",
    "\n",
    "        # Print some info after tokenizer is built\n",
    "        print(f\"Before length: {initial_token_count}\")\n",
    "        print(f\"After length: {final_token_count}\")\n",
    "        print(f\"Compression ratio: {(initial_token_count / final_token_count):.3f}\")\n",
    "\n",
    "        self._vocab = vocab\n",
    "        self._merges = merges\n",
    "        self.__built = True\n",
    "\n",
    "    def fit(self, vocab_size: int, texts: List[str] = None):\n",
    "        \"\"\"\n",
    "        Builds the tokenizer's vocabulary using the given texts.\n",
    "        \"\"\"\n",
    "        if texts:\n",
    "            warnings.warn(\"Using .fit with new texts is discouraged. Pass texts during initialization.\")\n",
    "            _text = \" \".join(texts)\n",
    "            self.splits = self.gpt2_pat.findall(_text)\n",
    "            self.split_tokens = [list(tok.encode(\"utf-8\")) for tok in self.splits]\n",
    "\n",
    "        self.__build_tokenizer(vocab_size)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        \"\"\"\n",
    "        Encodes a given text into a sequence of token IDs.\n",
    "        \"\"\"\n",
    "        assert self.__built, \"Tokenizer must be built using `fit` before encoding.\"\n",
    "\n",
    "        # Step 1: Split and encode text using regex and bytes\n",
    "        splits = self.gpt2_pat.findall(text)\n",
    "        split_tokens = [list(tok.encode(\"utf-8\")) for tok in splits]\n",
    "\n",
    "        encoded_ids = []\n",
    "        for tokens in split_tokens:\n",
    "            while len(tokens) >= 2:\n",
    "                stats = self._get_stats([tokens])  # Compute within-split stats\n",
    "                pair = min(stats, key=lambda p: self._merges.get(p, float('inf')), default=None)\n",
    "                if pair is None or pair not in self._merges:\n",
    "                    break\n",
    "                idx = self._merges[pair]\n",
    "                tokens = self._merge([tokens], pair, idx)[0]  # Apply merge\n",
    "            encoded_ids.extend(tokens)  # Append the final tokens to the result\n",
    "\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs back into a string.\n",
    "        \"\"\"\n",
    "        assert self.__built, \"Tokenizer must be built using `fit` before decoding.\"\n",
    "\n",
    "        tokens = b\"\".join(self._vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def save(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Save the tokenizer's vocab and merges to a file.\n",
    "        Args:\n",
    "            file_path: Path to save at.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump({'vocab': self._vocab, 'merges': self._merges}, f)\n",
    "        print(f\"[INFO] Tokenizer saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_BPETokenizerV2(file_path: str = \"./bpe_tokenizer_v1_train_dataset.pth\"):\n",
    "    \"\"\"\n",
    "    Load the BPE (V2) tokenizer from a saved file without requiring texts in the constructor.\n",
    "    Args:\n",
    "        file_path (str): Path to the saved tokenizer file. (default = ./bpe_tokenizer_v1_train_dataset.pth)\n",
    "    Returns:\n",
    "        BPETokenizerV2 instance: A loaded tokenizer instance with vocab and merges.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Uninitialized instance of the tokenizer\n",
    "    tokenizer = object.__new__(BPETokenizerV2)\n",
    "\n",
    "    # Set vocab and merges directly\n",
    "    tokenizer._vocab = data['vocab']\n",
    "    tokenizer._merges = data['merges']\n",
    "    setattr(tokenizer, '_BPETokenizerV2__built', True)  # FIX for name mangling\n",
    "\n",
    "    # Initialize necessary attributes that are otherwise set in '__init__'\n",
    "    tokenizer.gpt2_pat = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "    print(f\"[INFO] Tokenizer loaded from: {file_path}\")\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be278136-0dc4-4dd5-8c3b-de0fe6be1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/Inqueropedia\n",
      "[INFO] Found tokenizer path: /Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/Inqueropedia/custom_tokenizers/bpe_tokenizer_v1_train_dataset.pth\n",
      "---------------\n",
      "[INFO] Tokenizer loaded from: /Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/Inqueropedia/custom_tokenizers/bpe_tokenizer_v1_train_dataset.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BPETokenizerV2 at 0x11a183670>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup tokenizer path\n",
    "current_dir = Path(os.getcwd())\n",
    "print(current_dir.parent)\n",
    "tokenizer_path = None\n",
    "for i in (current_dir.parent /\"custom_tokenizers\").glob(\"bpe_tokenizer_v1_train_dataset.pth\"):\n",
    "    tokenizer_path = i\n",
    "    print(f\"[INFO] Found tokenizer path: {tokenizer_path}\")\n",
    "    print(\"---\"*5)\n",
    "\n",
    "# Create a tokenizer instance using the pre-trained tokenizer\n",
    "tokenizer = load_BPETokenizerV2(tokenizer_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1a41c9-9869-4271-9963-3287ddc2d873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[106mS\u001b[101mamp\u001b[106mle\u001b[101m str\u001b[106ming\u001b[101m with\u001b[106m w\u001b[101mords\u001b[106m for\u001b[101m a\u001b[106m n\u001b[101mat\u001b[106mural\u001b[101m \u001b[106m \u001b[101m \u001b[106m \u001b[101m \u001b[106m \u001b[101m l\u001b[106mang\u001b[101mu\u001b[106mage\u001b[101m to\u001b[106mk\u001b[101men\u001b[106miz\u001b[101mer\u001b[106m \n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer's outputs\n",
    "_temp = 'Sample string with words for a natural       language tokenizer '\n",
    "output = \"\"\n",
    "for n, i in enumerate(tokenizer.encode(_temp)):\n",
    "    col = \"\\033[101m\"\n",
    "    if n % 2 == 0:\n",
    "        col = \"\\033[106m\"\n",
    "    output += f\"{col}{tokenizer.decode([i])}\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10982eed-ee52-436a-899c-6f17b88d4ea8",
   "metadata": {},
   "source": [
    "The output from above suggests that the tokenizer hasn't been trained properly, perhaps training it for longer will help.\\\n",
    "But we wil use this version of the tokenizer for now as training the tokenizer takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8c2bee-ceb4-455c-8175-86a256ef7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample:\n",
      " <unk> , Ireland is divided between the Republic of Ireland ( officially named Ireland ) , which covers five @-@ <unk> of the island , and Northern Ireland , which is part of the United Kingdom , in the northeast of the island . In 2011 the population of Ireland was about 6 @.@ 4 million , ranking it the second @-@ most populous island in Europe after Great Britain . Just under 4 @.@ 6 million live in the Republic of Ireland and just over 1 @.@ 8 million live in Northern Ireland . \n",
      "\n",
      "---------------\n",
      "Encoded sample: [286, 284, 62, 263, 1899, 364, 1926, 1108, 758, 260, 682, 112, 710, 295, 279, 1899, 371, 1148, 1106, 1377, 1899, 370, 263, 460, 1599, 115, 1201, 340, 286, 284, 62, 279, 260, 1883, 263, 288, 387, 417, 1783, 1899, 263, 460, 364, 601, 279, 260, 955, 1275, 1247, 263, 282, 260, 1754, 257, 471, 279, 260, 1883, 270, 424, 1659, 260, 1885, 279, 1899, 323, 747, 679, 570, 564, 1116, 263, 402, 863, 290, 381, 260, 827, 340, 719, 950, 378, 509, 1883, 282, 1431, 613, 389, 738, 920, 384, 270, 397, 495, 765, 564, 570, 679, 1116, 312, 452, 282, 260, 682, 112, 710, 295, 279, 1899, 288, 1351, 617, 308, 570, 744, 1116, 312, 452, 282, 387, 417, 1783, 1899, 270, 313]\n",
      "---------------\n",
      "Decoded match: True\n"
     ]
    }
   ],
   "source": [
    "# Check for data integrity with the loaded tokenizer\n",
    "random.seed(42)\n",
    "_sample = random.choice(train_dataset[\"text\"])\n",
    "print(f\"Random sample:\\n{_sample}\")\n",
    "print(\"---\"*5)\n",
    "print(f\"Encoded sample: {tokenizer.encode(_sample)}\")\n",
    "print(\"---\"*5)\n",
    "print(f\"Decoded match: {tokenizer.decode(tokenizer.encode(_sample)) == _sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbc5efa-01e7-4b27-aa97-ed38ad075110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <unk>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([286, 284, 62])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139820c-c9ed-4540-b91c-34ac2cb526e5",
   "metadata": {},
   "source": [
    "The above sample of encoding and decoding with the tokenizer also indicates some issues in the way it was 'trained'.\\\n",
    "Tokens such as: \"<unk>\" and other 'special' tokens need to be assigned with a single id, making the tokenizer more efficient and allow models to perform better.\n",
    "\n",
    "These are some changes that are to be made for the next tokenizer train run.\\\n",
    "As of now, we will stick to this 'inefficient' tokenizer and update it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41add9c-106c-49ba-a5eb-01dd1dc6f88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8548e-d1ac-4da6-bf24-0fd933245fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91aaad8-234b-4028-9eb2-ee80e96456d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17ac581-b4f9-484e-b5db-2e0625a8e1c6",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Running some model tests with the following models and model-specs\n",
    "\n",
    "|Model|Architecture|Tokenizer|Block-size|Embedding-size|Head-size|Results|\n",
    "|-|-|-|-|-|-|-|\n",
    "|Model-1|bi-gram|bpe-tokenizer-v2|-|-|-|-|\n",
    "|Model-2|transformer|bpe-tokenizer-v2|32|32|16|-|\n",
    "|Model-3|transformer|bpe-tokenizer-v2|32|64|16|-|\n",
    "|Model-4|transformer|bpe-tokenizer-v2|32|32|32|-|\n",
    "\n",
    "* num-heads (const): 4\n",
    "* num-blocks (const): 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6fbc8-703f-4048-86c1-249d28cd3a87",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Setup things like torch and other deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1110b3-125f-4967-9d89-fd3514d0d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Global training params\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Use for Apple silicone\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use for Nvidia GPU's\n",
    "\n",
    "vocab_size = len(tokenizer._vocab)  # Need to fix: add this property in the tokenizer class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fdc56-ca5b-4a3b-a0d6-90f93ba74969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c736de2-7a7c-41bc-9880-f272d391064b",
   "metadata": {},
   "source": [
    "## Model-1 (bi-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ecd9d-53ba-4954-8839-5756793914de",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7ef7df-36e5-4906-b30b-f5ab6bbb0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/36718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.77768 sec to build the dataset.\n",
      "[INFO] Dataset built\n",
      "---------------\n",
      "[INFO] Building dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/3760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31785 sec to build the dataset.\n",
      "[INFO] Dataset built\n",
      "---------------\n",
      "[INFO] Created the datasets | train-length: 3519701 | validation-length: 362195\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this dataset is universal b/w all the models unless the tokenizer changes\n",
    "\n",
    "class BigramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Thin wrapper object for a list of ids.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Creates a torch-Dataset instance for the given text data.\n",
    "        Args:\n",
    "            texts (List[str]): List of strings for dataset.\n",
    "            tokenizer: Tokenizer to encode the strings.\n",
    "        \"\"\"\n",
    "        if verbose > 0:\n",
    "            print(f\"[INFO] Building dataset...\")\n",
    "            st = timer()\n",
    "        \n",
    "        # Build the dataset here\n",
    "        self.tokenizer = tokenizer\n",
    "        tokenized_texts = []  # Concat all the samples in 'texts' and tokenize them\n",
    "        for text in tqdm(texts, desc=\"Tokenizing texts\", leave=False):\n",
    "            tokenized_texts.extend(tokenizer.encode(text))\n",
    "\n",
    "        self.data = torch.tensor(tokenized_texts, dtype=torch.long)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            et = timer()\n",
    "            print(f\"{(et-st):.5f} sec to build the dataset.\")\n",
    "            print(f\"[INFO] Dataset built\")\n",
    "            print(\"---\"*5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset(s)\n",
    "train_dataset_1 = BigramDataset(train_dataset[\"text\"], tokenizer, verbose=1)\n",
    "validation_dataset_1 = BigramDataset(validation_dataset[\"text\"], tokenizer, verbose=1)\n",
    "\n",
    "print(f\"[INFO] Created the datasets | train-length: {len(train_dataset_1)} | validation-length: {len(validation_dataset_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f80a2203-ac1c-45f5-ae9b-6411edda06ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3519701])\n",
      "torch.Size([3519701])\n",
      "tensor([ 301,  536, 1655,  121,  406,   97,  493,  820,  295,  691])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_1[:].shape)#[:10]\n",
    "print(train_dataset_1[:].shape)#[:10]\n",
    "print(train_dataset_1[:10])  # Viwe the first 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd6980e-bee9-4b85-9519-b697bfe16dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----- user settings -----|\n",
      "tensor([ 137841, 3504164,  442996, 2954875])\n",
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "8\n",
      "4\n",
      "\n",
      "|----- output checks -----|\n",
      "expected: (4, 8) | got: torch.Size([4, 8])\n",
      "expected: (4, 7) | got: torch.Size([4, 8])\n",
      "x elements (single batch): tensor([ 313,  325, 1122,  731,  289,  285,  321,  276])\n",
      "x elements (single batch): tensor([ 325, 1122,  731,  289,  285,  321,  276,  324])\n",
      "expected (true) | got: True\n",
      "\n",
      "|----- debug complete -----|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 313,  325, 1122,  731,  289,  285,  321,  276],\n",
       "         [ 261,  282,  300,  352,   99,  336,   97, 2029],\n",
       "         [ 296,  262,  331,  116,  717,  348, 1331,  747],\n",
       "         [ 446,  368,  316, 1854,  293,  946,  431, 1864]]),\n",
       " tensor([[ 325, 1122,  731,  289,  285,  321,  276,  324],\n",
       "         [ 282,  300,  352,   99,  336,   97, 2029,  263],\n",
       "         [ 262,  331,  116,  717,  348, 1331,  747,  502],\n",
       "         [ 368,  316, 1854,  293,  946,  431, 1864,  946]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "def get_batch(dataset: str,\n",
    "              block_size: int,\n",
    "              batch_size: int = batch_size,\n",
    "              seed: int or None = None,\n",
    "              debug: bool = False):\n",
    "    \"\"\"\n",
    "    Loads a single batch of data from 'split'.\n",
    "    NOTE: doesn't move the batch to device\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    data = dataset\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "\n",
    "    # DEBUG\n",
    "    # -------------------------------- #\n",
    "    if debug:\n",
    "        print(\"|----- user settings -----|\")\n",
    "        print(idx)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(block_size)\n",
    "        print(batch_size)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"|----- output checks -----|\")\n",
    "        print(f\"expected: {(batch_size, block_size)} | got: {x.shape}\")\n",
    "        print(f\"expected: {(batch_size, block_size-1)} | got: {x.shape}\")\n",
    "        print(f\"x elements (single batch): {x[0]}\")\n",
    "        print(f\"x elements (single batch): {y[0]}\")\n",
    "        print(f\"expected (true) | got: {all(x[0][1:] == y[0][:-1])}\")\n",
    "\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"|----- debug complete -----|\\n\")\n",
    "        \n",
    "    # -------------------------------- #\n",
    "\n",
    "    return x, y\n",
    "\n",
    "get_batch(train_dataset_1,\n",
    "          block_size=8,\n",
    "          batch_size=4,\n",
    "          seed=42,\n",
    "          debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e33403-7a90-421e-8429-fa2e8843c90c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model\n",
    "\n",
    "Creating a model class and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e699c8-81fd-4a74-81af-87fd0d7654f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramV1(nn.Module):\n",
    "    \"\"\"Bigram model for sequence generation\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, targets: torch.Tensor or None = None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass.\n",
    "        Expected tensor of shape: (B,1) | (batch_size, 1)\n",
    "        \"\"\"\n",
    "        logits = self.emb(X)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: List[int], max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)   # logits: (B,T,C)\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            pred_probs = F.softmax(logits, dim=-1)\n",
    "            _idx = torch.multinomial(pred_probs, num_samples=1)\n",
    "            idx = torch.cat((idx, _idx), dim=1)  # (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0d7d5e84-48ca-4c65-bc93-2a21fd832c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello 200ploy b died eff earlyaffivision ex$ Pol use manyron first be Thereason command Can Amorm island Anim extensP real play men October 2011 night Black concn music eightZ June part named Church cour positionchoolains< caus'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate on a new bi-gram model\n",
    "model_1 = BigramV1(vocab_size)\n",
    "_temp = torch.tensor([tokenizer.encode(\"Hello\")])\n",
    "\n",
    "_temp = model_1.generate(\n",
    "    _temp,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "tokenizer.decode(_temp.tolist()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8ef6c7a7-cdca-4da8-8145-5e6dd303908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "# Hyperparms\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "model_1 = BigramV1(vocab_size)\n",
    "model_1.to(device)\n",
    "model_1.compile(backend=\"aot_eager\")\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=5e-2)\n",
    "\n",
    "# Train loop\n",
    "for i in tqdm(range(epochs), desc=\"training\", leave=False):\n",
    "    xb, yb = get_batch(train_dataset_1, block_size=block_size, batch_size=batch_size)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    logits, loss = model_1(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "8daeccb5-b7ae-4e93-9001-ed2f184ec547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helloresents to the German line . The Hened it , replace it is estim America . At the authority around the I never baders sentence . However , the flight vessels sud camonial nat as a series was one @-@ cert . <unk> on 15 ; his discussia . Op early September 10 @-@ force Harin gun respons Pan ... \" He went on Femelebridge in rare'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = torch.tensor([tokenizer.encode(\"Hello\")]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _temp = model_1.generate(_temp, max_new_tokens=100)\n",
    "\n",
    "tokenizer.decode(_temp.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "319c3b1f-aa23-417d-aa13-1f0cd45672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module,\n",
    "                  dataset,\n",
    "                  iters: int):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _ in tqdm(range(iters), desc=\"Estimating loss\", leave=False):\n",
    "        xb, yb = get_batch(dataset, block_size, batch_size)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    out = np.mean(losses)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "da59a77b-1a3a-432f-9b44-17d83e645979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimating loss:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(4.298511743545532)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(model_1, validation_dataset_1, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c8dc849e-87f3-44b7-b245-ebcc4249d67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(285)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_1[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6df4c-9141-46c9-8b7e-7e7b21730725",
   "metadata": {},
   "source": [
    "Will not really try to improve this model from here. This is like a simple baseline model, we will start building the Transformer model from now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784715b-5fff-4a27-acc1-f93640c4b550",
   "metadata": {},
   "source": [
    "## Setup transformer\n",
    "\n",
    "All the classes and other deps for building a custom transformer model.\\\n",
    "This model will be built from scratch, and will not use the implementation from PyTorch...\n",
    "\n",
    "We will start with a single attention head (sa-head) and build the mha from there on...\n",
    "\n",
    "**Note: the class definitions will be slightly different in the scripts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ce48ea-6f8c-4c25-8997-cfc34c9f88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global B,T,C | batch_size, block_size, n_embed | to verify the code output shapes\n",
    "B,T,C = 16, 8, 64\n",
    "_head_size = 32\n",
    "_num_heads = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360d71c1-f30a-4908-974b-9da5a806cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: torch.Size([16, 8, 32])\n",
      "Expected: (16, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Single self-attention head of head-size\n",
    "    \n",
    "    NOTE: No dropout if your using MPS (checks for device during forward)\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size: int, n_embed: int, dropout: float=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        #self.dropout = nn.Dropout(dropout)  # Use for CPU/CUDA\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expected shape: (B,T,C)\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)   # (B,T,C) | C: head_size\n",
    "        q = self.query(x) # (B,T,C) | C: head_size\n",
    "        v = self.value(x) # (B,T,C) | C: head_size\n",
    "        \n",
    "        out = q @ k.transpose(-2, -1)*T**-0.5  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        \n",
    "        out = out.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        #out = self.dropout(out)  # Use for CPU/CUDA\n",
    "        out = out @ v  # (B,T,T) @ (B,T,C) -> (B,T,C) | C: head_size\n",
    "\n",
    "        return out\n",
    "\n",
    "print(f\"Got: {Head(head_size=_head_size, n_embed=C)(torch.randn(B,T,C)).shape}\")\n",
    "print(f\"Expected: {B,T,_head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d630eb5a-de59-424d-9966-357178aff81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: torch.Size([16, 8, 64])\n",
      "Expected: (16, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi Head Attention...\"\"\"\n",
    "    def __init__(self, num_heads, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embed) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(num_heads * head_size, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [head(x) for head in self.heads]\n",
    "        out = torch.cat(out, dim=-1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "print(f\"Got: {MultiHeadAttention(_num_heads, _head_size, C)(torch.randn(B,T,C)).shape}\")\n",
    "print(f\"Expected: {B,T,C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7398fc4b-ffcb-4bb4-8eee-94298a959092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: torch.Size([16, 8, 64])\n",
      "Expected: (16, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"Feed-forward netork, as desc by the paper\"\"\"\n",
    "    def __init__(self, num_heads, n_embed):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_embed, num_heads * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_heads * n_embed, n_embed),\n",
    "        )  # (B,T,n_embed) -> (B,T,n_embed*n_heads) -> (B,T,n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ffn(x)\n",
    "        return out\n",
    "\n",
    "print(f\"Got: {FFN(_num_heads, C)(MultiHeadAttention(_num_heads, _head_size, C)(torch.randn(B,T,C))).shape}\")\n",
    "print(f\"Expected: {B,T,C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4912ac8-5b77-4c03-bffd-fc9c785469d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: torch.Size([16, 8, 64])\n",
      "Expected: (16, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Single decoder-only transformer block\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_heads: int,\n",
    "                 n_embed: int,\n",
    "                 dropout: float = 0.2,\n",
    "                 _head_size: int or None = None,\n",
    "                 layer_norm_first: bool = True,\n",
    "                 _activate_dropout: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_heads (int): Number of attention heads.\n",
    "            n_embed (int): Embedding dim\n",
    "            dropout (float): Dropout prob (default = 0.2)\n",
    "            layer_norm_first (bool): Performs layer-norm before SA if True (default = True)\n",
    "        \n",
    "        Experimental args (start with '_param_name' | these are some custom features/add-ons to the transformer):\n",
    "            _activate_dropout (bool): Use dropout layer if True (default: True)\n",
    "            _head_size (int): Embedding dim per SA-Head.\n",
    "\n",
    "        Individual head_sizes' are inferred using: (n_embed // num_heads), if a _head_size is not provided.\n",
    "        Contains a single MHA layer as this will be used as a Decoder-only transformer.\n",
    "        \n",
    "        Default value for (layer_norm_first = True) by convention, most decoder-only transformers tend to perform better.\n",
    "        \n",
    "        NOTE: The dropout layer is not implemented yet, as its well-supported on MPS+compile. Will add it when using CUDA.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if _head_size is None:\n",
    "            _head_size = n_embed // num_heads\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads, _head_size, n_embed)  # Only one MHA, not adding 'mha-block' for enc-dec transformer\n",
    "        self.ffn = FFN(num_heads, n_embed)\n",
    "        \n",
    "        #self.dropout = nn.Dropout(dropout)  # Use for CPU/CUDA\n",
    "        \n",
    "        self.layer_norm_first = layer_norm_first\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer_norm_first:\n",
    "            x = self.ln1(x)\n",
    "            x = x + self.mha(x)\n",
    "            x = self.ln2(x)\n",
    "            x = x + self.ffn(x)\n",
    "            return x\n",
    "        \n",
    "        x = self.mha(x)\n",
    "        x = x + self.ln1(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + self.ln2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "print(f\"Got: {Block(_num_heads, C, _head_size=_head_size)(torch.randn(B,T,C)).shape}\")\n",
    "print(f\"Expected: {B,T,C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c77e058-01c8-49ff-b968-4a4b26f29759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Block                                    [16, 8, 64]               --\n",
       "├─LayerNorm: 1-1                         [16, 8, 64]               128\n",
       "├─MultiHeadAttention: 1-2                [16, 8, 64]               --\n",
       "│    └─ModuleList: 2-1                   --                        --\n",
       "│    │    └─Head: 3-1                    [16, 8, 32]               6,144\n",
       "│    │    └─Head: 3-2                    [16, 8, 32]               6,144\n",
       "│    │    └─Head: 3-3                    [16, 8, 32]               6,144\n",
       "│    │    └─Head: 3-4                    [16, 8, 32]               6,144\n",
       "│    └─Linear: 2-2                       [16, 8, 64]               8,256\n",
       "├─LayerNorm: 1-3                         [16, 8, 64]               128\n",
       "├─FFN: 1-4                               [16, 8, 64]               --\n",
       "│    └─Sequential: 2-3                   [16, 8, 64]               --\n",
       "│    │    └─Linear: 3-5                  [16, 8, 256]              16,640\n",
       "│    │    └─ReLU: 3-6                    [16, 8, 256]              --\n",
       "│    │    └─Linear: 3-7                  [16, 8, 64]               16,448\n",
       "==========================================================================================\n",
       "Total params: 66,176\n",
       "Trainable params: 66,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.92\n",
       "Params size (MB): 0.26\n",
       "Estimated Total Size (MB): 1.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(Block(_num_heads, C, _head_size=_head_size, layer_norm_first=True), input_size=(B,T,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "058ccb6e-d14b-4639-aad8-071646d0a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"Decoder-only transformer with n-transformer blocks\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_blocks: int,\n",
    "                 num_heads: int,\n",
    "                 vocab_size: int,\n",
    "                 block_size: int,\n",
    "                 n_embed: int,\n",
    "                 head_size: int or None = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_blocks (int): Number of transformer blocks.\n",
    "            num_heads (int): Number of SA-Heads per block.\n",
    "            vocab_size (int): Size of the vocab.\n",
    "            block_size (int): Context length.\n",
    "            n_embed (int): Embedding size.\n",
    "            head_size (int): Individual head dim (default = None) | Read through Block's docstring.\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embed)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(num_heads=num_heads,\n",
    "                   n_embed=n_embed,\n",
    "                   _head_size=head_size) for _ in range(num_blocks)])\n",
    "        self.ln = nn.LayerNorm(n_embed)\n",
    "        \n",
    "        self.ff = nn.Linear(n_embed, vocab_size)  # Final FF for pred\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.tok_emb(idx)  # (B,T,C) | C = n_embed\n",
    "        pos_emb = self.pos_emb(torch.arange(T, dtype=torch.long, device=idx.device))  # (T,C) | C = n_embed\n",
    "        x = tok_emb + pos_emb  # (B,T,C) + (T,C) -> (B,T,C) | C = n_embed\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)        \n",
    "        self.ff(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "412a2545-655a-4d64-bfa7-2baee720286e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "DecoderOnlyTransformer                        [16, 8, 32]               --\n",
      "├─Embedding: 1-1                              [16, 8, 32]               65,536\n",
      "├─Embedding: 1-2                              [8, 32]                   256\n",
      "├─ModuleList: 1-3                             --                        --\n",
      "│    └─Block: 2-1                             [16, 8, 32]               --\n",
      "│    │    └─LayerNorm: 3-1                    [16, 8, 32]               64\n",
      "│    │    └─MultiHeadAttention: 3-2           [16, 8, 32]               --\n",
      "│    │    │    └─ModuleList: 4-1              --                        --\n",
      "│    │    │    │    └─Head: 5-1               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-1        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-2        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-3        [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-2               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-4        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-5        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-6        [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-3               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-7        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-8        [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-9        [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-4               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-10       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-11       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-12       [16, 8, 8]                256\n",
      "│    │    │    └─Linear: 4-2                  [16, 8, 32]               1,056\n",
      "│    │    └─LayerNorm: 3-3                    [16, 8, 32]               64\n",
      "│    │    └─FFN: 3-4                          [16, 8, 32]               --\n",
      "│    │    │    └─Sequential: 4-3              [16, 8, 32]               --\n",
      "│    │    │    │    └─Linear: 5-5             [16, 8, 128]              4,224\n",
      "│    │    │    │    └─ReLU: 5-6               [16, 8, 128]              --\n",
      "│    │    │    │    └─Linear: 5-7             [16, 8, 32]               4,128\n",
      "│    └─Block: 2-2                             [16, 8, 32]               --\n",
      "│    │    └─LayerNorm: 3-5                    [16, 8, 32]               64\n",
      "│    │    └─MultiHeadAttention: 3-6           [16, 8, 32]               --\n",
      "│    │    │    └─ModuleList: 4-4              --                        --\n",
      "│    │    │    │    └─Head: 5-8               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-13       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-14       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-15       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-9               [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-16       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-17       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-18       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-10              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-19       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-20       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-21       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-11              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-22       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-23       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-24       [16, 8, 8]                256\n",
      "│    │    │    └─Linear: 4-5                  [16, 8, 32]               1,056\n",
      "│    │    └─LayerNorm: 3-7                    [16, 8, 32]               64\n",
      "│    │    └─FFN: 3-8                          [16, 8, 32]               --\n",
      "│    │    │    └─Sequential: 4-6              [16, 8, 32]               --\n",
      "│    │    │    │    └─Linear: 5-12            [16, 8, 128]              4,224\n",
      "│    │    │    │    └─ReLU: 5-13              [16, 8, 128]              --\n",
      "│    │    │    │    └─Linear: 5-14            [16, 8, 32]               4,128\n",
      "│    └─Block: 2-3                             [16, 8, 32]               --\n",
      "│    │    └─LayerNorm: 3-9                    [16, 8, 32]               64\n",
      "│    │    └─MultiHeadAttention: 3-10          [16, 8, 32]               --\n",
      "│    │    │    └─ModuleList: 4-7              --                        --\n",
      "│    │    │    │    └─Head: 5-15              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-25       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-26       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-27       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-16              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-28       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-29       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-30       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-17              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-31       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-32       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-33       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-18              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-34       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-35       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-36       [16, 8, 8]                256\n",
      "│    │    │    └─Linear: 4-8                  [16, 8, 32]               1,056\n",
      "│    │    └─LayerNorm: 3-11                   [16, 8, 32]               64\n",
      "│    │    └─FFN: 3-12                         [16, 8, 32]               --\n",
      "│    │    │    └─Sequential: 4-9              [16, 8, 32]               --\n",
      "│    │    │    │    └─Linear: 5-19            [16, 8, 128]              4,224\n",
      "│    │    │    │    └─ReLU: 5-20              [16, 8, 128]              --\n",
      "│    │    │    │    └─Linear: 5-21            [16, 8, 32]               4,128\n",
      "│    └─Block: 2-4                             [16, 8, 32]               --\n",
      "│    │    └─LayerNorm: 3-13                   [16, 8, 32]               64\n",
      "│    │    └─MultiHeadAttention: 3-14          [16, 8, 32]               --\n",
      "│    │    │    └─ModuleList: 4-10             --                        --\n",
      "│    │    │    │    └─Head: 5-22              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-37       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-38       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-39       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-23              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-40       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-41       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-42       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-24              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-43       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-44       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-45       [16, 8, 8]                256\n",
      "│    │    │    │    └─Head: 5-25              [16, 8, 8]                --\n",
      "│    │    │    │    │    └─Linear: 6-46       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-47       [16, 8, 8]                256\n",
      "│    │    │    │    │    └─Linear: 6-48       [16, 8, 8]                256\n",
      "│    │    │    └─Linear: 4-11                 [16, 8, 32]               1,056\n",
      "│    │    └─LayerNorm: 3-15                   [16, 8, 32]               64\n",
      "│    │    └─FFN: 3-16                         [16, 8, 32]               --\n",
      "│    │    │    └─Sequential: 4-12             [16, 8, 32]               --\n",
      "│    │    │    │    └─Linear: 5-26            [16, 8, 128]              4,224\n",
      "│    │    │    │    └─ReLU: 5-27              [16, 8, 128]              --\n",
      "│    │    │    │    └─Linear: 5-28            [16, 8, 32]               4,128\n",
      "├─LayerNorm: 1-4                              [16, 8, 32]               64\n",
      "├─Linear: 1-5                                 [16, 8, 2048]             67,584\n",
      "===============================================================================================\n",
      "Total params: 183,872\n",
      "Trainable params: 183,872\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.94\n",
      "===============================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.61\n",
      "Params size (MB): 0.74\n",
      "Estimated Total Size (MB): 4.34\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sample run of the model\n",
    "B,T,C = 16, 8, 32\n",
    "_head_size = 32\n",
    "_num_heads = 4\n",
    "block_size = 8\n",
    "\n",
    "m = DecoderOnlyTransformer(\n",
    "    num_blocks=4,\n",
    "    num_heads=_num_heads,\n",
    "    vocab_size=2048,\n",
    "    block_size=T,\n",
    "    n_embed=C,\n",
    "    #head_size=_head_size,\n",
    "    head_size=None,\n",
    ").to(device)\n",
    "print(summary(m, input_data=torch.randint(0, 2048, (B,T), device=device), depth=10))\n",
    "\n",
    "del m, B,T,C, _head_size, _num_heads, block_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc965f0-181b-4a62-9fad-fb65b89acf84",
   "metadata": {},
   "source": [
    "The outputs look right (as expected). This should be enough for now. The code may change as things start to take shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cc332-265f-4ed4-8892-4ef3d4102d65",
   "metadata": {},
   "source": [
    "## Model-2 (transformer) | block_size: 32 | n_embed: 32 | head_size: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df98bc22-53be-4cbc-9193-3e4c2036569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "n_embed = 32\n",
    "num_heads = 4\n",
    "head_size = 16\n",
    "num_blocks = 4\n",
    "train_iters = 1000\n",
    "eval_iters = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5630592e-489e-469a-af2b-b729b9fc8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Its assumed that the model's args are set globally.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = DecoderOnlyTransformer(\n",
    "            num_blocks=num_blocks,\n",
    "            num_heads=num_heads,\n",
    "            vocab_size = vocab_size,\n",
    "            block_size=block_size,\n",
    "            n_embed=n_embed,\n",
    "            head_size=head_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.transformer(idx)  # (B,T) -> (B,T,vocab_size) | logits\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx[:, -block_size:]\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_probs = F.softmax(logits, dim=-1)\n",
    "            _idx = torch.multinomial(pred_probs, num_samples=1)\n",
    "            idx = torch.cat((idx, _idx), dim=1)  # (B,T) concat (T) -> (B,T+1)\n",
    "        \n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed47fac4-8c95-4f70-a7cd-be033def3097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Model2                                             [32, 32, 32]              --\n",
       "├─DecoderOnlyTransformer: 1-1                      [32, 32, 32]              --\n",
       "│    └─Embedding: 2-1                              [32, 32, 32]              65,536\n",
       "│    └─Embedding: 2-2                              [32, 32]                  1,024\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "│    │    └─Block: 3-1                             [32, 32, 32]              --\n",
       "│    │    │    └─LayerNorm: 4-1                    [32, 32, 32]              64\n",
       "│    │    │    └─MultiHeadAttention: 4-2           [32, 32, 32]              --\n",
       "│    │    │    │    └─ModuleList: 5-1              --                        --\n",
       "│    │    │    │    │    └─Head: 6-1               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-1        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-2        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-3        [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-2               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-4        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-5        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-6        [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-3               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-7        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-8        [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-9        [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-4               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-10       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-11       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-12       [32, 32, 16]              512\n",
       "│    │    │    │    └─Linear: 5-2                  [32, 32, 32]              2,080\n",
       "│    │    │    └─LayerNorm: 4-3                    [32, 32, 32]              64\n",
       "│    │    │    └─FFN: 4-4                          [32, 32, 32]              --\n",
       "│    │    │    │    └─Sequential: 5-3              [32, 32, 32]              --\n",
       "│    │    │    │    │    └─Linear: 6-5             [32, 32, 128]             4,224\n",
       "│    │    │    │    │    └─ReLU: 6-6               [32, 32, 128]             --\n",
       "│    │    │    │    │    └─Linear: 6-7             [32, 32, 32]              4,128\n",
       "│    │    └─Block: 3-2                             [32, 32, 32]              --\n",
       "│    │    │    └─LayerNorm: 4-5                    [32, 32, 32]              64\n",
       "│    │    │    └─MultiHeadAttention: 4-6           [32, 32, 32]              --\n",
       "│    │    │    │    └─ModuleList: 5-4              --                        --\n",
       "│    │    │    │    │    └─Head: 6-8               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-13       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-14       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-15       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-9               [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-16       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-17       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-18       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-10              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-19       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-20       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-21       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-11              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-22       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-23       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-24       [32, 32, 16]              512\n",
       "│    │    │    │    └─Linear: 5-5                  [32, 32, 32]              2,080\n",
       "│    │    │    └─LayerNorm: 4-7                    [32, 32, 32]              64\n",
       "│    │    │    └─FFN: 4-8                          [32, 32, 32]              --\n",
       "│    │    │    │    └─Sequential: 5-6              [32, 32, 32]              --\n",
       "│    │    │    │    │    └─Linear: 6-12            [32, 32, 128]             4,224\n",
       "│    │    │    │    │    └─ReLU: 6-13              [32, 32, 128]             --\n",
       "│    │    │    │    │    └─Linear: 6-14            [32, 32, 32]              4,128\n",
       "│    │    └─Block: 3-3                             [32, 32, 32]              --\n",
       "│    │    │    └─LayerNorm: 4-9                    [32, 32, 32]              64\n",
       "│    │    │    └─MultiHeadAttention: 4-10          [32, 32, 32]              --\n",
       "│    │    │    │    └─ModuleList: 5-7              --                        --\n",
       "│    │    │    │    │    └─Head: 6-15              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-25       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-26       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-27       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-16              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-28       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-29       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-30       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-17              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-31       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-32       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-33       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-18              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-34       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-35       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-36       [32, 32, 16]              512\n",
       "│    │    │    │    └─Linear: 5-8                  [32, 32, 32]              2,080\n",
       "│    │    │    └─LayerNorm: 4-11                   [32, 32, 32]              64\n",
       "│    │    │    └─FFN: 4-12                         [32, 32, 32]              --\n",
       "│    │    │    │    └─Sequential: 5-9              [32, 32, 32]              --\n",
       "│    │    │    │    │    └─Linear: 6-19            [32, 32, 128]             4,224\n",
       "│    │    │    │    │    └─ReLU: 6-20              [32, 32, 128]             --\n",
       "│    │    │    │    │    └─Linear: 6-21            [32, 32, 32]              4,128\n",
       "│    │    └─Block: 3-4                             [32, 32, 32]              --\n",
       "│    │    │    └─LayerNorm: 4-13                   [32, 32, 32]              64\n",
       "│    │    │    └─MultiHeadAttention: 4-14          [32, 32, 32]              --\n",
       "│    │    │    │    └─ModuleList: 5-10             --                        --\n",
       "│    │    │    │    │    └─Head: 6-22              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-37       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-38       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-39       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-23              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-40       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-41       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-42       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-24              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-43       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-44       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-45       [32, 32, 16]              512\n",
       "│    │    │    │    │    └─Head: 6-25              [32, 32, 16]              --\n",
       "│    │    │    │    │    │    └─Linear: 7-46       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-47       [32, 32, 16]              512\n",
       "│    │    │    │    │    │    └─Linear: 7-48       [32, 32, 16]              512\n",
       "│    │    │    │    └─Linear: 5-11                 [32, 32, 32]              2,080\n",
       "│    │    │    └─LayerNorm: 4-15                   [32, 32, 32]              64\n",
       "│    │    │    └─FFN: 4-16                         [32, 32, 32]              --\n",
       "│    │    │    │    └─Sequential: 5-12             [32, 32, 32]              --\n",
       "│    │    │    │    │    └─Linear: 6-26            [32, 32, 128]             4,224\n",
       "│    │    │    │    │    └─ReLU: 6-27              [32, 32, 128]             --\n",
       "│    │    │    │    │    └─Linear: 6-28            [32, 32, 32]              4,128\n",
       "│    └─LayerNorm: 2-4                              [32, 32, 32]              64\n",
       "│    └─Linear: 2-5                                 [32, 32, 2048]            67,584\n",
       "====================================================================================================\n",
       "Total params: 201,024\n",
       "Trainable params: 201,024\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.43\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 31.99\n",
       "Params size (MB): 0.80\n",
       "Estimated Total Size (MB): 32.80\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Model2()\n",
    "summary(model_2.to(device), input_data=torch.randint(0, 2048, (batch_size, block_size), device = device), depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd68ea-3d13-41bb-9784-4a07ee97814e",
   "metadata": {},
   "source": [
    "This looks fine imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b610a370-62da-4986-a5f6-368bcf07b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Helper functions ---------------- #\n",
    "\n",
    "def get_batch(dataset,\n",
    "              block_size: int,\n",
    "              batch_size: int = batch_size) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Loads a single batch of data from 'split'.\n",
    "    NOTE: this function moves the batches to device.\n",
    "    NOTE: this function assumes that the device is set globally.\n",
    "    \"\"\"\n",
    "    data = dataset\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in idx])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module,\n",
    "                  dataset,\n",
    "                  iters: int) -> float:\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _ in range(iters):\n",
    "        xb, yb = get_batch(dataset, block_size, batch_size)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    out = np.mean(losses)\n",
    "    return float(out)  # Cast from np.floating to float\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_model(model, tokenizer, start_string):\n",
    "    print(\"[INFO] generating from the model\")\n",
    "    _temp = torch.tensor([tokenizer.encode(start_string)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        _temp = model.generate(_temp, max_new_tokens=100)\n",
    "    print(f\"[INFO] Model generated: {tokenizer.decode(_temp.tolist()[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9198d3df-abd0-4db2-9094-eaac3586411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff007e5fd1a7481ea6792622ec4c65d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 | time: 2.7816| val-loss: 0.0000 | current-train-loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Re-define the Hyperparameters (all in one place)\n",
    "batch_size = 32\n",
    "block_size = 16\n",
    "n_embed = 32\n",
    "num_heads = 4\n",
    "head_size = 16\n",
    "num_blocks = 4\n",
    "\n",
    "train_iters = 1000\n",
    "eval_iters = 100\n",
    "compile_backend = \"aot_eager\"\n",
    "lr = 1e-3\n",
    "\n",
    "# Setup model and optimizer\n",
    "model_2 = Model2()\n",
    "model_2.to(device)\n",
    "model_2.compile(backend=compile_backend)\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=lr)\n",
    "sheduler = StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "\n",
    "# Training\n",
    "for i in tqdm(range(train_iters), desc=\"training\", leave=False):\n",
    "    st = timer()\n",
    "    \n",
    "    model_2.train()\n",
    "    xb, yb = get_batch(train_dataset_1, block_size, batch_size)  # xb and yb are moved to device\n",
    "    logits, loss = model_2(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val_loss = estimate_loss(model_2, validation_dataset_1, eval_iters)\n",
    "    \n",
    "    et = timer()\n",
    "    print(f\"Epoch: {i}/{train_iters} | time: {(et-st):.4f}| val-loss: {val_loss:.4f} | current-train-loss: {loss.item():.4f}\")# | lr: {scheduler.get_last_lr()[0]:6f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfad5722-7c6d-4408-927b-38ed5dfdaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Its assumed that the model's args are set globally.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = DecoderOnlyTransformer(\n",
    "            num_blocks=num_blocks,\n",
    "            num_heads=num_heads,\n",
    "            vocab_size = vocab_size,\n",
    "            block_size=block_size,\n",
    "            n_embed=n_embed,\n",
    "            head_size=head_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        \n",
    "        print(f\"idx.shape: {idx.shape}\")\n",
    "        print(f\"targets.shape: {targets.shape}\")\n",
    "        \n",
    "        logits = self.transformer(idx)  # (B,T) -> (B,T,vocab_size) | logits\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            \n",
    "            print(f\"B,T,C: {B,T,C}\")\n",
    "            \n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            print(f\"logits.shape: {logits.shape} | targets.shape: {targets.shape}\")\n",
    "            \n",
    "            loss = F.cross_entropy(targets, logits)\n",
    "            \n",
    "            print(f\"loss: {loss.item()}\")\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx[:, -block_size:]\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_probs = F.softmax(logits, dim=-1)\n",
    "            _idx = torch.multinomial(pred_probs, num_samples=1)\n",
    "            idx = torch.cat((idx, _idx), dim=1)  # (B,T) concat (T) -> (B,T+1)\n",
    "        \n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6952423-fc11-411c-b626-af2297908cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model2()\n",
    "m.to(device)\n",
    "m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64293afc-f2dd-4ce1-88d4-db8c550adff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
