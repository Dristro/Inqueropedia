{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2af58e9-4e4e-455c-8e0f-694c89ad3553",
   "metadata": {},
   "source": [
    "# ChatBot-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bf3fdf-0aab-4fe3-9fbb-7ae74a432f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f4ffd-6d7d-4e6b-8109-5b83a9f3a531",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfe482-7aa4-4f83-945c-2ab85c5ec84e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca9a8a4-642a-4cb2-836c-d6b230d23de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup dataset viarables and methods for data import\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.path.dirname(current_dir), \"datasets\", \"plain_text_wikipedia\", \"AllCombined.txt\")  # Path to all data\n",
    "\n",
    "def get_lines(file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_path: path to file (.txt file only)\n",
    "    Returns:\n",
    "        all the lines found in given file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd52d711-265a-4eb8-841a-b07a3dd56ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random line from the dataset:\n",
      "Some people say that art is a product or item that is made with the intention of stimulating the human senses as well as the human mind, spirit and soul. An artwork is normally judged by how much impact it has on people, the number of people who can relate to it, and how much they appreciate it. Some people also get inspired.\n",
      "\n",
      "Number of lines: 2052699\n"
     ]
    }
   ],
   "source": [
    "### Importing and storing the data into a variable\n",
    "all_data = get_lines(data_dir)\n",
    "print(f\"Random line from the dataset:\\n{all_data[60]}\")  # Fixed position to 60 | many times the line is \"\\n\"\n",
    "print(f\"Number of lines: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054446a-d508-4e43-9e92-d3a5b465b45f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Split and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d883ea44-884d-4dd4-afa6-5a690fe08c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1436889\n",
      "Test samples: 307905\n",
      "Validation samples: 307905\n",
      "True\n",
      "Overlap train-test: 2454\n",
      "Overlap train-validation: 2520\n",
      "Overlap validation-test: 988\n"
     ]
    }
   ],
   "source": [
    "### Split the data into train-test-validation splits\n",
    "def get_splits(target_data,\n",
    "               train_size: float = 0.7,\n",
    "               test_size: float = 0.15,\n",
    "               validation_size: float = 0.15,\n",
    "               shuffle: bool = True,\n",
    "               seed: int = 42):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        target_data: dataset to split on\n",
    "        train_size: size of the train dataset\n",
    "        test_size: size of the test dataset\n",
    "        validation_size: size of the validation dataset\n",
    "        shuffle: shuffle the target_dataset before spliting\n",
    "    Returns:\n",
    "        train_split, test_split, validation_split\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Setup\n",
    "    _sum = train_size+test_size+validation_size\n",
    "    assert _sum == 1, f\"Sum of all sizes must be 1 got {sum}\"\n",
    "    random.seed(seed)\n",
    "    if shuffle:\n",
    "        random.shuffle(target_data)\n",
    "\n",
    "    # Get split idx's\n",
    "    total_length = len(target_data)\n",
    "    train_end_idx = int(total_length * train_size)\n",
    "    test_end_idx = train_end_idx + int(total_length * test_size) + 1\n",
    "    validation_end_idx = test_end_idx + int(total_length * validation_size) + 1\n",
    "    \n",
    "    # Split the data into train-test-validation\n",
    "    train_split = target_data[:train_end_idx]\n",
    "    test_split = target_data[train_end_idx:test_end_idx]\n",
    "    validation_split = target_data[test_end_idx:validation_end_idx]\n",
    "\n",
    "    return train_split, test_split, validation_split\n",
    "\n",
    "def validate_splits(train_data, test_data, validation_data, leak_ok: bool = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        leak_ok: set to False to check for data-leakage, set to True otherwise\n",
    "        *All other arguments have same meaning as their name\n",
    "    \"\"\"\n",
    "    _train_set = set(train_data)\n",
    "    _test_set = set(test_data)\n",
    "    _validation_set = set(validation_data)\n",
    "    overlap_train_test = _train_set.intersection(_test_set)\n",
    "    overlap_train_validation = _train_set.intersection(_validation_set)\n",
    "    overlap_validation_test = _validation_set.intersection(_test_set)\n",
    "    print(f\"Overlap train-test: {len(overlap_train_test)}\")\n",
    "    print(f\"Overlap train-validation: {len(overlap_train_validation)}\")\n",
    "    print(f\"Overlap validation-test: {len(overlap_validation_test)}\")\n",
    "    _sum = len(overlap_train_test)+len(overlap_train_validation)+len(overlap_validation_test)\n",
    "    if not leak_ok:\n",
    "        assert _sum == 0, f\"There exists a data-leakage. Found '{_sum}' samples overlaping.\"\n",
    "    return overlap_train_test, overlap_train_validation, overlap_validation_test\n",
    "\n",
    "\n",
    "\n",
    "# Get splits\n",
    "train_data, test_data, validation_data = get_splits(\n",
    "    target_data=all_data,\n",
    "    # Leaving all as default\n",
    ")\n",
    "\n",
    "# Get some info about the splits\n",
    "_sum = len(train_data)+len(test_data)+len(validation_data)  # sum of splits\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Validation samples: {len(validation_data)}\")\n",
    "print(f\"{_sum==len(all_data)}\")\n",
    "# Check for data leakages\n",
    "overlaps = validate_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    validation_data=validation_data,\n",
    "    leak_ok=True, # We may have some leaks atm from chars like \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f18ed-d641-4449-b934-22493bc54cb5",
   "metadata": {},
   "source": [
    "### Understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633639e8-fecd-48a1-a1cf-19582ba39e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436889, 307905, 307905)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check the data split's lengths\n",
    "len(train_data), len(test_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fc9f80-2cf9-41b4-9a83-cc28924acbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get vocab\n",
    "# Create a general purpose tokenizer\n",
    "class Tokenizer():\n",
    "    def __init__(self, token_size):\n",
    "        \"\"\"\n",
    "        Creates an instance of a tokenizer.\n",
    "        You can use this object to convert raw data into tokens.\n",
    "\n",
    "        Args:\n",
    "            token_size: size of each token\n",
    "        \"\"\"\n",
    "        self.token_size = token_size\n",
    "        self._vocab = None\n",
    "        self._stoi = None\n",
    "        self._itos = None\n",
    "\n",
    "    def _preprocess(self,\n",
    "                    data: list,\n",
    "                    strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Preprocesses the data to handle punctuation and normalize tokens.\n",
    "\n",
    "        Args:\n",
    "            data: list of sentences or lines to preprocess\n",
    "        Returns:\n",
    "            A list of preprocessed sentences where punctuation is separated.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        processed_data = []\n",
    "        \n",
    "        # Get rid of puntuation if needed\n",
    "        if strip_punctuation:\n",
    "            for line in data:\n",
    "                line = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', line)\n",
    "                processed_data.append(line)\n",
    "        # Add spaces around punctuation, otherwise\n",
    "        else:\n",
    "            for line in data:\n",
    "                line = re.sub(r\"([.,!?;:])\", r\" \\1 \", line)  # Add spaces around punctuation\n",
    "                line = re.sub(r\"\\s+\", \" \", line).strip()  # Remove extra spaces\n",
    "                processed_data.append(line)\n",
    "        \n",
    "        return processed_data\n",
    "            \n",
    "        \n",
    "    \n",
    "    def _token_split(self,\n",
    "                     data: list,\n",
    "                     strip_punctuation):\n",
    "        \"\"\"\n",
    "        Tokenizes the data on the token_size and returns a list of the data, where each entry has token_size number of words.\n",
    "        \n",
    "        Args:\n",
    "            data: list of sentences, words, etc to tokenize\n",
    "        Returns:\n",
    "            A list, where each entry has 'token_size' number of words\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        data = self._preprocess(data=data, strip_punctuation=strip_punctuation)\n",
    "        for line in data:\n",
    "            words = line.split()\n",
    "            for i in range(0, len(words), self.token_size):\n",
    "                tokens.append(\" \".join(words[i:i + self.token_size]))\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self,\n",
    "                   data: list,\n",
    "                   strip_punctuation: bool = False):\n",
    "        \"\"\"\n",
    "        Produces a vocab list, on the given tokens.\n",
    "        \n",
    "        Args:\n",
    "            data: list of sentences to build the vocab on\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "             vocab: list of unique tokens\n",
    "             stoi: dict mapping strings to indices\n",
    "             itos: dict mapping indices to strings\n",
    "        \"\"\"\n",
    "        tokens = self._token_split(data, strip_punctuation=strip_punctuation)\n",
    "        self._vocab = sorted(set(tokens))\n",
    "        self._stoi = {token: idx for idx, token in enumerate(self._vocab)}\n",
    "        self._itos = {idx: token for token, idx in self._stoi.items()}\n",
    "        \n",
    "        return self._vocab, self._stoi, self._itos\n",
    "\n",
    "    def tokenize_string(self,\n",
    "                 data: list,\n",
    "                 strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes the given data into tokens based onthe prebuild vocab.\n",
    "\n",
    "        Args:\n",
    "            data: data to tokenize\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "            List of tokens converted to indices\n",
    "        \"\"\"\n",
    "        assert self._stoi is not None, f\"Vocab is not built yet. Call .build_vocab first.\"\n",
    "        tokens = self._token_split(data, strip_punctuation=strip_punctuation)\n",
    "        token_indices = [self._stoi[token] for token in tokens if token in self._stoi]\n",
    "        \n",
    "        return token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525c9526-4b0f-432a-b954-3ba213b17f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup and run the tokenizer\n",
    "TOKEN_SIZE = 1  # Word level (token_size: number of words per token)\n",
    "\n",
    "\"\"\"\n",
    "# Testing the tokenizer\n",
    "test_tokenizer = Tokenizer(1)\n",
    "sample_data = [\"This is the first set of tokens.\", \"And this is the second set of tokens\"]\n",
    "#print(test_tokenizer._token_split(sample_data))\n",
    "tokens, vocab, stoi, itos = test_tokenizer.tokenize(data = sample_data, strip_punctuation=False)\n",
    "\n",
    "print(tokens)\n",
    "print(\"------\")\n",
    "print(vocab)\n",
    "print(\"------\")\n",
    "print(stoi)\n",
    "print(\"------\")\n",
    "print(itos)\n",
    "print(\"------\")\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_token_size_1 = Tokenizer(TOKEN_SIZE)\n",
    "vocab, stoi, itos = tokenizer_token_size_1.build_vocab(\n",
    "    data=train_data,\n",
    "    strip_punctuation=False\n",
    ")\n",
    "train_tokens = tokenizer_token_size_1.tokenize_string(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818b711-e9c8-4c45-818a-44e54a52a9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_kernel)",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
