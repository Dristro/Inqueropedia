{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2af58e9-4e4e-455c-8e0f-694c89ad3553",
   "metadata": {},
   "source": [
    "# ChatBot-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bf3fdf-0aab-4fe3-9fbb-7ae74a432f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f4ffd-6d7d-4e6b-8109-5b83a9f3a531",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfe482-7aa4-4f83-945c-2ab85c5ec84e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca9a8a4-642a-4cb2-836c-d6b230d23de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup dataset viarables and methods for data import\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.path.dirname(current_dir), \"datasets\", \"plain_text_wikipedia\", \"AllCombined.txt\")  # Path to all data\n",
    "\n",
    "def get_lines(file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_path: path to file (.txt file only)\n",
    "    Returns:\n",
    "        all the lines found in given file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd52d711-265a-4eb8-841a-b07a3dd56ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random line from the dataset:\n",
      "Some people say that art is a product or item that is made with the intention of stimulating the human senses as well as the human mind, spirit and soul. An artwork is normally judged by how much impact it has on people, the number of people who can relate to it, and how much they appreciate it. Some people also get inspired.\n",
      "\n",
      "Number of lines: 2052699\n"
     ]
    }
   ],
   "source": [
    "### Importing and storing the data into a variable\n",
    "all_data = get_lines(data_dir)\n",
    "print(f\"Random line from the dataset:\\n{all_data[60]}\")  # Fixed position to 60 | many times the line is \"\\n\"\n",
    "print(f\"Number of lines: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054446a-d508-4e43-9e92-d3a5b465b45f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Split and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d883ea44-884d-4dd4-afa6-5a690fe08c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1436889\n",
      "Test samples: 307905\n",
      "Validation samples: 307905\n",
      "True\n",
      "Overlap train-test: 2454\n",
      "Overlap train-validation: 2520\n",
      "Overlap validation-test: 988\n"
     ]
    }
   ],
   "source": [
    "### Split the data into train-test-validation splits\n",
    "def get_splits(target_data,\n",
    "               train_size: float = 0.7,\n",
    "               test_size: float = 0.15,\n",
    "               validation_size: float = 0.15,\n",
    "               shuffle: bool = True,\n",
    "               seed: int = 42):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        target_data: dataset to split on\n",
    "        train_size: size of the train dataset\n",
    "        test_size: size of the test dataset\n",
    "        validation_size: size of the validation dataset\n",
    "        shuffle: shuffle the target_dataset before spliting\n",
    "    Returns:\n",
    "        train_split, test_split, validation_split\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Setup\n",
    "    _sum = train_size+test_size+validation_size\n",
    "    assert _sum == 1, f\"Sum of all sizes must be 1 got {sum}\"\n",
    "    random.seed(seed)\n",
    "    if shuffle:\n",
    "        random.shuffle(target_data)\n",
    "\n",
    "    # Get split idx's\n",
    "    total_length = len(target_data)\n",
    "    train_end_idx = int(total_length * train_size)\n",
    "    test_end_idx = train_end_idx + int(total_length * test_size) + 1\n",
    "    validation_end_idx = test_end_idx + int(total_length * validation_size) + 1\n",
    "    \n",
    "    # Split the data into train-test-validation\n",
    "    train_split = target_data[:train_end_idx]\n",
    "    test_split = target_data[train_end_idx:test_end_idx]\n",
    "    validation_split = target_data[test_end_idx:validation_end_idx]\n",
    "\n",
    "    return train_split, test_split, validation_split\n",
    "\n",
    "def validate_splits(train_data, test_data, validation_data, leak_ok: bool = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        leak_ok: set to False to check for data-leakage, set to True otherwise\n",
    "        *All other arguments have same meaning as their name\n",
    "    \"\"\"\n",
    "    _train_set = set(train_data)\n",
    "    _test_set = set(test_data)\n",
    "    _validation_set = set(validation_data)\n",
    "    overlap_train_test = _train_set.intersection(_test_set)\n",
    "    overlap_train_validation = _train_set.intersection(_validation_set)\n",
    "    overlap_validation_test = _validation_set.intersection(_test_set)\n",
    "    print(f\"Overlap train-test: {len(overlap_train_test)}\")\n",
    "    print(f\"Overlap train-validation: {len(overlap_train_validation)}\")\n",
    "    print(f\"Overlap validation-test: {len(overlap_validation_test)}\")\n",
    "    _sum = len(overlap_train_test)+len(overlap_train_validation)+len(overlap_validation_test)\n",
    "    if not leak_ok:\n",
    "        assert _sum == 0, f\"There exists a data-leakage. Found '{_sum}' samples overlaping.\"\n",
    "    return overlap_train_test, overlap_train_validation, overlap_validation_test\n",
    "\n",
    "\n",
    "\n",
    "# Get splits\n",
    "train_data, test_data, validation_data = get_splits(\n",
    "    target_data=all_data,\n",
    "    # Leaving all as default\n",
    ")\n",
    "\n",
    "# Get some info about the splits\n",
    "_sum = len(train_data)+len(test_data)+len(validation_data)  # sum of splits\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Validation samples: {len(validation_data)}\")\n",
    "print(f\"{_sum==len(all_data)}\")\n",
    "# Check for data leakages\n",
    "overlaps = validate_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    validation_data=validation_data,\n",
    "    leak_ok=True, # We may have some leaks atm from chars like \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f18ed-d641-4449-b934-22493bc54cb5",
   "metadata": {},
   "source": [
    "### Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674d43c-b95c-46ee-bd8b-ed6ed4f3f5c1",
   "metadata": {},
   "source": [
    "Creating the **Tokenizer** and **Text-dataset** classes. \\\n",
    "These will be used to create PyTorch friendly datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633639e8-fecd-48a1-a1cf-19582ba39e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436889, 307905, 307905)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check the data split's lengths\n",
    "len(train_data), len(test_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55fc9f80-2cf9-41b4-9a83-cc28924acbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get vocab\n",
    "# Create a general purpose tokenizer\n",
    "class Tokenizer():\n",
    "    def __init__(self, token_size):\n",
    "        \"\"\"\n",
    "        Creates an instance of a tokenizer.\n",
    "        You can use this object to convert raw data into tokens.\n",
    "\n",
    "        Args:\n",
    "            token_size: size of each token\n",
    "        \"\"\"\n",
    "        self.token_size = token_size\n",
    "        self._vocab = None\n",
    "        self._stoi = None\n",
    "        self._itos = None\n",
    "\n",
    "    def _preprocess(self,\n",
    "                    text: list,\n",
    "                    strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Preprocesses a string to handle punctuation and normalize tokens.\n",
    "\n",
    "        Args:\n",
    "            text: a string to preprocess\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing\n",
    "        Returns:\n",
    "            Preprocessed string\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import string\n",
    "\n",
    "        # Strip punctuation if required\n",
    "        if strip_punctuation:\n",
    "            text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n",
    "        else:\n",
    "            # Add spaces around punctuation\n",
    "            text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    def _token_split(self,\n",
    "                     text: str,\n",
    "                     strip_punctuation) -> list:\n",
    "        \"\"\"\n",
    "        Tokenize a single input string into chunks of token_size words.\n",
    "        \n",
    "        Args:\n",
    "            text: a string to tokenize\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing\n",
    "        Returns:\n",
    "            A list, where each entry has 'token_size' number of words\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Preprocess the string\n",
    "        text = self._preprocess(text=text, strip_punctuation=strip_punctuation)\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), self.token_size):\n",
    "            tokens.append(\" \".join(words[i:i + self.token_size]))\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self,\n",
    "                   data: list,\n",
    "                   strip_punctuation: bool = False):\n",
    "        \"\"\"\n",
    "        Produces a vocab list, on the given tokens.\n",
    "        \n",
    "        Args:\n",
    "            data: list of sentences to build the vocab on\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "             vocab: list of unique tokens\n",
    "             stoi: dict mapping strings to indices\n",
    "             itos: dict mapping indices to strings\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for sentence in data:\n",
    "            tokens.extend(self._token_split(sentence, strip_punctuation=strip_punctuation))\n",
    "        self._vocab = sorted(set(tokens))\n",
    "        self._stoi = {token: idx for idx, token in enumerate(self._vocab)}\n",
    "        self._itos = {idx: token for token, idx in self._stoi.items()}\n",
    "        \n",
    "        return self._vocab, self._stoi, self._itos\n",
    "\n",
    "    def tokenize_string(self,\n",
    "                 text: str,\n",
    "                 strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes the given string into tokens based on the prebuilt vocab.\n",
    "\n",
    "        Args:\n",
    "            text: string to tokenize\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "            List of tokens converted to indices\n",
    "        \"\"\"\n",
    "        assert self._stoi is not None, f\"Vocab is not built yet. Call .build_vocab first.\"\n",
    "        tokens = self._token_split(text=text, strip_punctuation=strip_punctuation)\n",
    "        token_indices = [self._stoi[token] for token in tokens if token in self._stoi]\n",
    "        \n",
    "        return token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "752762af-01e9-42a5-ad1e-36b9d8839d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 tokenizer,\n",
    "                 strip_punctuation = False,\n",
    "                 max_len = None):\n",
    "        \"\"\"\n",
    "        Custom dataset for tokenized text.\n",
    "        Builds a tokenized dataset on provided data, using the specified tokenizer.\n",
    "\n",
    "        Args:\n",
    "            data: list of sentences\n",
    "            tokenizer: instance of the Tokenizer class\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing.\n",
    "            max_len: optional, maximum length of tokenized sequences (for padding).\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.strip_punctuation = strip_punctuation\n",
    "        self.max_len = max_len\n",
    "        self.tokenized_data = [self.tokenizer.tokenize_string(line, strip_punctuation) for line in self.data]\n",
    "        if self.max_len:\n",
    "            self.tokenized_data = [tokens[:self.max_len] for tokens in self.tokenized_data] # Truncate\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of samples in \"data\"\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tokenized sample from \"data\" at \"idx\".\n",
    "\n",
    "        Args:\n",
    "            idx: index of sample to retrieve\n",
    "        Returns:\n",
    "            Sample at \"idx\", tokenized.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenized_data[idx]\n",
    "        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7818b711-e9c8-4c45-818a-44e54a52a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer hyperparams\n",
    "TOKEN_SIZE = 1\n",
    "STRIP_PUNCTUATION = False\n",
    "\n",
    "# Tokenizer instance (and setup)\n",
    "tokenizer = Tokenizer(TOKEN_SIZE)\n",
    "vocab, stoi, itos = tokenizer.build_vocab(\n",
    "    data=train_data,\n",
    "    strip_punctuation=STRIP_PUNCTUATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8e198d5-f3fb-473a-b077-7934f8c41455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data/strings\n",
    "sample_data = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Another example, with punctuation!\",\n",
    "    \"Let's see if this works well.\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "# Tokenize sample_data\n",
    "sample_data_tokenized = tokenizer.tokenize_string(text=sample_data[0])\n",
    "print(sample_data_tokenized)\n",
    "for i in sample_data_tokenized:\n",
    "    print(itos[i])\n",
    "\"\"\"\n",
    "\n",
    "dataset = TextDataset(data=sample_data, tokenizer=tokenizer, strip_punctuation=False, max_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b769c8c2-7900-43ea-a9c9-6226995ea293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9cf0434-c2f6-45a8-8b33-49881762d95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([162081, 534850, 120790, 633093, 593438,      0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26ccb004-ede0-49fc-bab7-0ee87c92cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another\n",
      "example\n",
      ",\n",
      "with\n",
      "punctuation\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.__getitem__(1):\n",
    "    print(itos[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be767b4-dddb-435e-97a5-7efedabe24e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_kernel)",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
