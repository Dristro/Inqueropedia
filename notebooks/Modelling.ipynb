{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4d80b1-6481-46c3-b416-3335a60bc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path(os.getcwd()).parent  # Get current dir (.parent, for root dir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e45401c-7da6-4ec7-ba22-3ca36c780a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April (Apr.) is the fourth month of the year i...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April always begins on the same day of the wee...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April comes between March and May, making it t...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April begins on the same day of the week as Ju...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In common years, April starts on the same day ...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count\n",
       "0  April (Apr.) is the fourth month of the year i...          31\n",
       "1  April always begins on the same day of the wee...          30\n",
       "2  April comes between March and May, making it t...          40\n",
       "3  April begins on the same day of the week as Ju...          51\n",
       "4  In common years, April starts on the same day ...         103"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the current state of the dataset into a df\n",
    "data_dir = current_dir / \"datasets\" / \"preprocessing\"\n",
    "data_path = data_dir / \"english_lang.csv\"\n",
    "all_data_df = pd.read_csv(data_path)\n",
    "all_data_df.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis=1, inplace=True)# Droping some cols (due to a bug in dataset creation)\n",
    "\n",
    "all_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20044f1c-89c5-4068-8861-bbf86d9f969f",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc299f-341e-403c-90eb-2bcc1f9ecd52",
   "metadata": {},
   "source": [
    "## Data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146699d-8e0b-44ad-8211-f880c51cde20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901218d2-fb76-4291-9bb6-93ddd26d548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the data into train-test-validation splits\n",
    "def get_splits(target_data: list,\n",
    "               train_size: float = 0.7,\n",
    "               test_size: float = 0.15,\n",
    "               validation_size: float = 0.15,\n",
    "               shuffle: bool = True,\n",
    "               seed: int = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Splits target_data into train-val-test datasets.\n",
    "    \n",
    "    Args:\n",
    "        target_data: dataset to split on\n",
    "        train_size: size of the train dataset\n",
    "        test_size: size of the test dataset\n",
    "        validation_size: size of the validation dataset\n",
    "        shuffle: shuffle the target_dataset before spliting\n",
    "        seed: control randomness in shuffle (default = None)\n",
    "    Returns:\n",
    "        train_split, test_split, validation_split\n",
    "    \"\"\"\n",
    "    _total = train_size+test_size+validation_size\n",
    "    assert _total == 1, f\"Total size must be 1 (100%), got: {_total} ({_total*100}%)\"\n",
    "    import random\n",
    "\n",
    "    # Setup\n",
    "    _sum = train_size+test_size+validation_size\n",
    "    assert _sum == 1, f\"Sum of all sizes must be 1 got {sum}\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    if shuffle:\n",
    "        random.shuffle(target_data)\n",
    "\n",
    "    # Get split idx's\n",
    "    total_length = len(target_data)\n",
    "    train_end_idx = int(total_length * train_size)\n",
    "    test_end_idx = train_end_idx + int(total_length * test_size) + 1\n",
    "    validation_end_idx = test_end_idx + int(total_length * validation_size) + 1\n",
    "\n",
    "    # Split the data into train-test-validation\n",
    "    train_split = target_data[:train_end_idx]\n",
    "    test_split = target_data[train_end_idx:test_end_idx]\n",
    "    validation_split = target_data[test_end_idx:validation_end_idx]\n",
    "\n",
    "    return train_split, test_split, validation_split\n",
    "\n",
    "def validate_splits(train_data: list,\n",
    "                    test_data: list,\n",
    "                    validation_data: list,\n",
    "                    leak_ok: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Validates the datasplits by counting the number of overlapping/shared samples between datasets.\n",
    "    \n",
    "    Args:\n",
    "        leak_ok: set to False to check for data-leakage, set to True otherwise\n",
    "        *All other arguments have same meaning as their name\n",
    "    Returns:\n",
    "        samples that overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    _train_set = set(train_data)\n",
    "    _test_set = set(test_data)\n",
    "    _validation_set = set(validation_data)\n",
    "    \n",
    "    overlap_train_test = _train_set.intersection(_test_set)\n",
    "    overlap_train_validation = _train_set.intersection(_validation_set)\n",
    "    overlap_validation_test = _validation_set.intersection(_test_set)\n",
    "    \n",
    "    print(f\"Overlap train-test: {len(overlap_train_test)}\")\n",
    "    print(f\"Overlap train-validation: {len(overlap_train_validation)}\")\n",
    "    print(f\"Overlap validation-test: {len(overlap_validation_test)}\")\n",
    "    \n",
    "    _sum = len(overlap_train_test)+len(overlap_train_validation)+len(overlap_validation_test)\n",
    "    \n",
    "    if leak_ok == False and _sum > 0:\n",
    "        print(f\"[!!!CRITICAL!!!] There exists a data-leakage. Found '{_sum}' samples overlaping.\")\n",
    "    \n",
    "    return overlap_train_test, overlap_train_validation, overlap_validation_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3d2e39-13ad-473e-9413-10f1ff0ff2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 556689\n",
      "Test samples: 119291\n",
      "Validation samples: 119291\n",
      "True\n",
      "Overlap train-test: 1661\n",
      "Overlap train-validation: 1683\n",
      "Overlap validation-test: 665\n",
      "[!!!CRITICAL!!!] There exists a data-leakage. Found '4009' samples overlaping.\n"
     ]
    }
   ],
   "source": [
    "# Store all lines from df into arr\n",
    "all_data_list = all_data_df[\"text\"].tolist()\n",
    "\n",
    "# Get splits\n",
    "train_data, test_data, validation_data = get_splits(\n",
    "    target_data=all_data_list,\n",
    "    seed=42\n",
    "    # Leaving all as default\n",
    ")\n",
    "\n",
    "# Get some info about the splits\n",
    "_sum = len(train_data)+len(test_data)+len(validation_data)  # sum of splits\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Validation samples: {len(validation_data)}\")\n",
    "print(f\"{_sum==len(all_data_list)}\")\n",
    "\n",
    "# Check for data leakages\n",
    "overlaps = validate_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    validation_data=validation_data,\n",
    "    leak_ok=False,  # We may have some leaks atm from chars like \"\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e8ebea-b9ed-48a1-9b63-9fd8d096be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test: 1661 | Train-val: 1683 | Val-test: 665\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;section begin=qf3 /&gt;&lt;section end=qf3 /&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wave is a kind of oscillation (disturbance) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319||100\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7||0||3||0||0||0||10||0\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14||0\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0         <section begin=qf3 /><section end=qf3 />\\n\n",
       "1  A wave is a kind of oscillation (disturbance) ...\n",
       "2                                         319||100\\n\n",
       "3                          7||0||3||0||0||0||10||0\\n\n",
       "4                                            14||0\\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Understanding the overlaping texts\n",
    "# Get overlap counts\n",
    "print(f\"Train-test: {len(overlaps[0])} | Train-val: {len(overlaps[1])} | Val-test: {len(overlaps[2])}\")\n",
    "\n",
    "# Checking the overlaps\n",
    "train_val_overlaps_df = pd.DataFrame(data=overlaps[1], columns=[\"text\"])\n",
    "train_val_overlaps_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57257458-2fb6-4e0e-9d9f-101528969155",
   "metadata": {},
   "source": [
    "I think the issue is caused with my implementation of the `get_splits` function, so ill try to verify it using the **train-test split** function from **sklearn**.\\\n",
    "Though \"my\" implementation shouldn't cause any problems (from a logical perspective), lets just verify for the samke of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a02ea61-9f8a-458e-96d7-694c5027b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap train-test: 1754\n",
      "Overlap train-validation: 0\n",
      "Overlap validation-test: 0\n",
      "[!!!CRITICAL!!!] There exists a data-leakage. Found '1754' samples overlaping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split for train_data\n",
    "train_data, test_data = train_test_split(all_data_list, train_size=0.7, test_size=0.3, random_state=42)\n",
    "# Split for val and test\n",
    "validation_data, test_data = train_test_split(test_data, train_size=0.5, random_state=42)\n",
    "\n",
    "overlaps = validate_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    validation_data=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c8882-7da7-4be8-9619-1fa9491b94a2",
   "metadata": {},
   "source": [
    "So... looks like there are some samples that are leaking b/w the train-val-test datasets.\\\n",
    "The validate splits function isn't a problem here, as it just compares the \"set\" of all values from its args. I.e. there exists some kind of overlaping samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1b6d7-39c1-4ac9-8084-0c062084d0fd",
   "metadata": {},
   "source": [
    "### Analyse splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31a6bc-1452-4c08-be9a-bb63e5382171",
   "metadata": {},
   "source": [
    "Since the train-test-split function from sklearn resulted in a smaller number of overlaps, let use that as out reference to analyse the overlaping samples.\n",
    "\n",
    "*This has little to do with the way sklearn splits the data, its due to the way we have to use it to split the data into train, _ first then split _ into test and val. (couldn't think of a good var name for _)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b8656d-db3c-41f4-877c-5a3dfcf31390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0         <section begin=qf3 /><section end=qf3 />\\n\n",
      "1  In medicine \"epidemiology\" is the study of wha...\n",
      "2                                         319||100\\n\n",
      "3  Scientists say this frog is related to the bla...\n",
      "4                    139||8||13||2||23||1||175||11\\n\n",
      "5                                          524||33\\n\n",
      "6                                            14||0\\n\n",
      "7                             National Film Awards\\n\n",
      "8                                      Southampton\\n\n",
      "9                       Most people speak Finnish.\\n\n"
     ]
    }
   ],
   "source": [
    "### Analysis\n",
    "\n",
    "# Covert overlaps from tuple to a dict of df's\n",
    "if type(overlaps) == tuple:\n",
    "    overlaps = {\n",
    "        \"train_val\": pd.DataFrame(overlaps[0], columns=[\"text\"]),\n",
    "        \"train_test\": pd.DataFrame(overlaps[1], columns=[\"text\"]),\n",
    "        \"val_test\": pd.DataFrame(overlaps[2], columns=[\"text\"]),\n",
    "    }\n",
    "\n",
    "# View few overlaping samples\n",
    "print(overlaps[\"train_val\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eadaae5-792d-493d-9101-cb147b771883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([39947, 201048], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check number of occurences of samples in all_data_df\n",
    "\n",
    "# Check sample overlap\n",
    "overlaping_text = overlaps[\"train_val\"].loc[1][\"text\"]\n",
    "matches = all_data_df[\"text\"] == overlaping_text\n",
    "\n",
    "# Check number of overlaps\n",
    "count_matches = matches.sum() if isinstance(matches, pd.Series) else sum(matches)\n",
    "print(f\"Occurrences: {count_matches}\")\n",
    "\n",
    "# Get idx of overlap entry to understand context\n",
    "matches[matches].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dc5e9c-0f40-408d-9351-112ddcf61e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729 = 7*247, further check if 247 is a prime or not , if its not prime, continue the same steps of prime factorization for 247.\n",
      "\n",
      "1729 = 7*247 = 7*13*19, Notice that all the factors for the 1729 at this point are prime and no further factors are possible except 1, so further factorization should be stopped.\n",
      "\n",
      "\"Encore une fois\" is a 1997 song by the German DJ group Sash! and it features German singer Sabine Ohmes. It is the second single from Sash!'s first studio album \"It's My Life – The Album\" which was released on 25 August 1997.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "\"Ecuador\" is a 1997 song by the German DJ group Sash! and it features German DJ Adrian Rodriguez. It is the third single from Sash!'s first studio album \"It's My Life – The Album\" which was released on 25 August 1997.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "---------------\n",
      "\"Encore une fois\" is a 1997 song by the German DJ group Sash! and it features German singer Sabine Ohmes. It is the second single from Sash!'s first studio album \"It's My Life – The Album\" which was released on 25 August 1997.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "\"Ecuador\" is a 1997 song by the German DJ group Sash! and it features German DJ Adrian Rodriguez. It is the third single from Sash!'s first studio album \"It's My Life – The Album\" which was released on 25 August 1997.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "Beril Böcekler (born 7 February 2004, Ankara) is a Turkish swimmer. \n",
      "\n",
      "Böcekler represented Turkey at the 2018 European Juvenile Swimming Competition, 2018 Mediterranean Olympic Games and the 2020 Summer Olympics. Böcekler has the Turkish records in 400, 800 and 1500 m free-style.\n",
      "\n",
      "---------------\n",
      "York County is a county located in the north-central section of the U.S. state of South Carolina. As of the 2020 census, the population was 282,090. Its county seat is York, South Carolina, and its largest city is Rock Hill.\n",
      "\n",
      "Colour the World\n",
      "\n",
      "\"Colour the World\" is a 1999 song by the German DJ group Sash! and it features Swedish singer Dr. Alban. It is the fourth and final single from Sash!'s second studio album \"Life Goes On (Sash! album)\" which was released on 17 August 1998.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "Ma Baker (Sash! song)\n",
      "\n",
      "\"Ma Baker\" is a 1998 song by the German DJ group Sash!. When the song was released it was called Ma Baker but it was changed to Somebody Scream! Ma Baker\".\n",
      "\n",
      "---------------\n",
      "\"Ma Baker\" is a 1998 song by the German DJ group Sash!. When the song was released it was called Ma Baker but it was changed to Somebody Scream! Ma Baker\".\n",
      "\n",
      "Adelante (Sash! song)\n",
      "\n",
      "\"Adelante\" is a 1999 song by the German DJ group Sash! and it features Peter Faulhammer and German DJ Rodriguez. It is the first single from their third studio album \"Trilenium\" which was released on 22 May 2000.\n",
      "\n",
      "It was played a lot in clubs around the world and is one of Sash!'s biggest songs.\n",
      "\n",
      "Just Around the Hill\n",
      "\n",
      "\"Just Around The Hill\" is a 2000 song by the German DJ group Sash! and it features English singer Tina Cousins. It is the second single from Sash's third studio album \"Trilenium\" which was released on 22 May 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(524911-3, 524911+3):\n",
    "    print(all_data_df.loc[i][\"text\"])\n",
    "print(\"---\"*5)\n",
    "for i in range(524913-3, 524913+3):\n",
    "    print(all_data_df.loc[i][\"text\"])\n",
    "print(\"---\"*5)\n",
    "for i in range(525016-3, 525016+3):\n",
    "    print(all_data_df.loc[i][\"text\"])\n",
    "print(\"---\"*5)\n",
    "for i in range(525021-3, 525021+3):\n",
    "    print(all_data_df.loc[i][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec906c-59a0-4a12-a2e9-2ab132546e33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This shows that there are many places where the entries are repeating. This can be a problem with the dataset, or the way the data lines were split. But for now, since the samples are repeating around areas with similar context. We can drop the repeating samples.\n",
    "So the next step would be to clean the data again from repeating samples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d09dc1-060c-4710-bb06-8a6d8852f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples droped: 13511\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April (Apr.) is the fourth month of the year i...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April always begins on the same day of the wee...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April comes between March and May, making it t...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April begins on the same day of the week as Ju...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In common years, April starts on the same day ...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In years immediately before common years, Apri...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April is a spring month in the Northern Hemisp...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count\n",
       "0  April (Apr.) is the fourth month of the year i...          31\n",
       "1  April always begins on the same day of the wee...          30\n",
       "2  April comes between March and May, making it t...          40\n",
       "3  April begins on the same day of the week as Ju...          51\n",
       "4  In common years, April starts on the same day ...         103\n",
       "5  In years immediately before common years, Apri...          70\n",
       "6  April is a spring month in the Northern Hemisp...          30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting rid of all repeating samples in the dataset\n",
    "unique_df = all_data_df.drop_duplicates(subset=\"text\", keep=\"first\",)\n",
    "len_unique_df = len(unique_df)\n",
    "unique_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of samples droped: {len(all_data_df) - len_unique_df}\")\n",
    "\n",
    "unique_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803d2c25-7772-48b5-b614-21f9a37a1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved to: /Users/dhruvnandigam/Desktop/Dhruv/Programing/NN/Inqueropedia/datasets/preprocessing/unique_samples.csv\n"
     ]
    }
   ],
   "source": [
    "### Saving this df as a '.csv'\n",
    "save_path = data_dir / \"unique_samples.csv\"\n",
    "unique_df.to_csv(save_path)\n",
    "print(f\"[INFO] Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401cb5c-2043-4769-948a-902fe128a65c",
   "metadata": {},
   "source": [
    "### Final splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bfacff6-d42f-4407-b736-f49413d6e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap train-test: 0\n",
      "Overlap train-validation: 0\n",
      "Overlap validation-test: 0\n"
     ]
    }
   ],
   "source": [
    "# Create splits\n",
    "unique_list = unique_df[\"text\"].tolist()\n",
    "train_data, test_data, validation_data = get_splits(\n",
    "    target_data=unique_list,\n",
    "    train_size=0.7,\n",
    "    test_size=0.15,\n",
    "    validation_size=0.15,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Validate splits\n",
    "overlaps = validate_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    validation_data=validation_data,\n",
    "    leak_ok=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f2baaff-5dbb-4324-9df4-376f489d1a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547232, 117263, 117265)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(validation_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78817c2-f8f4-41ea-aeda-1888b832db50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a556cbe-79c4-4cf7-aeeb-1516843c14c2",
   "metadata": {},
   "source": [
    "## Model preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffe712-7f14-4d38-ad64-fd9389bda363",
   "metadata": {},
   "source": [
    "Creating the **Tokenizer** and **Text-dataset** classes. \\\n",
    "These will be used to create PyTorch friendly datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "929a7ab1-8600-4d28-8160-bc8e18d635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a general purpose tokenizer\n",
    "class Tokenizer():\n",
    "    def __init__(self, token_size):\n",
    "        \"\"\"\n",
    "        Creates an instance of a tokenizer.\n",
    "        You can use this object to convert raw data into tokens.\n",
    "\n",
    "        Args:\n",
    "            token_size: size of each token\n",
    "        \"\"\"\n",
    "        self.token_size = token_size\n",
    "        self._vocab = None\n",
    "        self._stoi = None\n",
    "        self._itos = None\n",
    "\n",
    "    def _preprocess(self,\n",
    "                    text: list,\n",
    "                    strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Preprocesses a string to handle punctuation and normalize tokens.\n",
    "\n",
    "        Args:\n",
    "            text: a string to preprocess\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing\n",
    "        Returns:\n",
    "            Preprocessed string\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import string\n",
    "\n",
    "        # Strip punctuation if required\n",
    "        if strip_punctuation:\n",
    "            text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n",
    "        else:\n",
    "            # Add spaces around punctuation\n",
    "            text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    def _token_split(self,\n",
    "                     text: str,\n",
    "                     strip_punctuation) -> list:\n",
    "        \"\"\"\n",
    "        Tokenize a single input string into chunks of token_size words.\n",
    "\n",
    "        Args:\n",
    "            text: a string to tokenize\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing\n",
    "        Returns:\n",
    "            A list, where each entry has 'token_size' number of words\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Preprocess the string\n",
    "        text = self._preprocess(text=text, strip_punctuation=strip_punctuation)\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), self.token_size):\n",
    "            tokens.append(\" \".join(words[i:i + self.token_size]))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self,\n",
    "                   data: list,\n",
    "                   strip_punctuation: bool = False):\n",
    "        \"\"\"\n",
    "        Produces a vocab list, on the given tokens.\n",
    "\n",
    "        Args:\n",
    "            data: list of sentences to build the vocab on\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "             vocab: list of unique tokens\n",
    "             stoi: dict mapping strings to indices\n",
    "             itos: dict mapping indices to strings\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for sentence in data:\n",
    "            tokens.extend(self._token_split(sentence, strip_punctuation=strip_punctuation))\n",
    "        self._vocab = sorted(set(tokens))\n",
    "        self._stoi = {token: idx for idx, token in enumerate(self._vocab)}\n",
    "        self._itos = {idx: token for token, idx in self._stoi.items()}\n",
    "\n",
    "        return self._vocab, self._stoi, self._itos\n",
    "\n",
    "    def tokenize_string(self,\n",
    "                 text: str,\n",
    "                 strip_punctuation = False) -> list:\n",
    "        \"\"\"\n",
    "        Tokenizes the given string into tokens based on the prebuilt vocab.\n",
    "\n",
    "        Args:\n",
    "            text: string to tokenize\n",
    "            strip_puntuation: where to srip punctuation during preprocessing\n",
    "        Returns:\n",
    "            List of tokens converted to indices\n",
    "        \"\"\"\n",
    "        assert self._stoi is not None, f\"Vocab is not built yet. Call .build_vocab first.\"\n",
    "        tokens = self._token_split(text=text, strip_punctuation=strip_punctuation)\n",
    "        token_indices = [self._stoi[token] for token in tokens if token in self._stoi]\n",
    "\n",
    "        return token_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba30970d-fa0b-41f0-ad27-03687daa055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a \"torch\" friendly dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 tokenizer,\n",
    "                 strip_punctuation = False,\n",
    "                 max_len = None):\n",
    "        \"\"\"\n",
    "        Custom dataset for tokenized text.\n",
    "        Builds a tokenized dataset on provided data, using the specified tokenizer.\n",
    "\n",
    "        Args:\n",
    "            data: list of sentences\n",
    "            tokenizer: instance of the Tokenizer class\n",
    "            strip_punctuation: whether to strip punctuation during preprocessing.\n",
    "            max_len: optional, maximum length of tokenized sequences (for padding).\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.strip_punctuation = strip_punctuation\n",
    "        self.max_len = max_len\n",
    "        self.tokenized_data = [self.tokenizer.tokenize_string(line, strip_punctuation) for line in self.data]\n",
    "        if self.max_len:\n",
    "            self.tokenized_data = [tokens[:self.max_len] for tokens in self.tokenized_data] # Truncate\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of samples in \"data\"\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tokenized sample from \"data\" at \"idx\".\n",
    "\n",
    "        Args:\n",
    "            idx: index of sample to retrieve\n",
    "        Returns:\n",
    "            Sample at \"idx\", tokenized.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenized_data[idx]\n",
    "        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f947977-bb78-4ccd-8679-74feeebd359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer hyperparams\n",
    "TOKEN_SIZE = 1\n",
    "STRIP_PUNCTUATION = False\n",
    "\n",
    "# Tokenizer instance (and setup)\n",
    "tokenizer = Tokenizer(TOKEN_SIZE)\n",
    "vocab, stoi, itos = tokenizer.build_vocab(\n",
    "    data=train_data,\n",
    "    strip_punctuation=STRIP_PUNCTUATION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f54ad10-97f6-4753-af22-d7690dff299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data/strings\n",
    "sample_data = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Another example, with punctuation!\",\n",
    "    \"Let's see if this works well.\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "# Tokenize sample_data\n",
    "sample_data_tokenized = tokenizer.tokenize_string(text=sample_data[0])\n",
    "print(sample_data_tokenized)\n",
    "for i in sample_data_tokenized:\n",
    "    print(itos[i])\n",
    "\"\"\"\n",
    "\n",
    "dataset = TextDataset(data=sample_data, tokenizer=tokenizer, strip_punctuation=False, max_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ae5b85c-8bcf-440d-90bc-493b84e2a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ee0a496-54b5-4898-9066-b1cf963cdce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([158247, 509836, 119183, 605606, 566985,      0])\n",
      "String: Another example , with punctuation !\n"
     ]
    }
   ],
   "source": [
    "# Print the 'tokenized' form of a sentence, and convert it back into its 'string' form\n",
    "print(f\"Tokens: {dataset.__getitem__(1)}\")\n",
    "print(f\"String: {' '.join(itos[i.item()] for i in dataset.__getitem__(1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f02f1c37-41c6-4eed-a41b-0364b156fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49387051-d19e-48db-b982-487cce7b2091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_kernel)",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
